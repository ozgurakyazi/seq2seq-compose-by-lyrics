{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preprocess import play_midi, parse_single_sentences, get_data_from_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains special words for the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_words = [\"<PAD>\", \"<GO>\", \"<END>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class For Dictionary\n",
    "The class contains the data and the helper functions for the dictionary to train seq2seq model.\n",
    "Most important functions are to mapping a list of words(a sentence) to a list of corresponding integer(indeces of words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Dictionary:\n",
    "    def __init__(self, sentences ):\n",
    "        self.word2index_map = dict()\n",
    "        self.index2word_map = dict()\n",
    "        self.init_register(sentences)\n",
    "        \n",
    "    # Initiates word2index_map and index2word_map\n",
    "    # Also extracts the max number of words in the sentences and saves it \n",
    "    def init_register(self,sentences):\n",
    "        global special_words\n",
    "        current_index = 0\n",
    "        ## save the maximum length among the sentences. \n",
    "        self.max_length = max([len(sentence) for sentence in sentences])+2\n",
    "        ### map special words, initially the mappings are empty.\n",
    "        for word in special_words:\n",
    "            self.word2index_map[word] = current_index\n",
    "            self.index2word_map[current_index] = word\n",
    "            current_index+=1\n",
    "        \n",
    "        s = set([item for sublist in sentences for item in sublist])\n",
    "        self.word2index_map.update({e:i+current_index for i,e in enumerate(s)})\n",
    "        self.index2word_map.update({v:k for k,v in self.word2index_map.items()})\n",
    "\n",
    "    \n",
    "    ## Returns the index of the word in the dictionary. It is assumed that the word\n",
    "    ## will be always in dictionary.\n",
    "    def get_index(self, word):\n",
    "        return self.word2index_map[word]\n",
    "    \n",
    "    ## Maps a sentence, which is a list of words, to the corresponding list of integers.\n",
    "    ## Each word is looked up from the map of the dictionary, and as in get_index method,\n",
    "    ## it is assumed that the word will always be found in the dictionary\n",
    "    def map_sentence(self, sentence):\n",
    "        return [self.get_index(i) for i in sentence if i in self.word2index_map]\n",
    "    \n",
    "    ## Returns the word by its index in dictionary.\n",
    "    def get_word(self, index):\n",
    "        return self.index2word_map[word]\n",
    "    \n",
    "    ## Pads the list of words to <PAD> at the end of the list of words in sentence\n",
    "    def pad_sentence(self, sentence):\n",
    "        return sentence + [\"<PAD>\"] * (self.max_length - len(sentence))\n",
    "    \n",
    "    ## Adds <GO> and <END> to the start and end of the sentence\n",
    "    def add_start_end_tokens(self, sentence):\n",
    "        return [\"<GO>\"] + sentence + [\"<END>\"]\n",
    "    \n",
    "    ## Transforms the sentence in a format suitable for the Neural Network\n",
    "    def transform_sentence(self, sentence):\n",
    "        s = self.pad_sentence(sentence)\n",
    "        s = self.add_start_end_tokens(s)\n",
    "        s = self.map_sentence(s)\n",
    "        return s\n",
    "    \n",
    "    ### Reverse operation of map_sentence. Gets the sentence, list of integers, as input and\n",
    "    ### maps each entry from index to word\n",
    "    #def decode_indeces(self,sentence_with_index):\n",
    "    #    sentence_list = list()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on test_midi_small/14_Years.mid\n",
      "Working on test_midi_small/All That She Wants.mid\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_data_from_dir(\"test_midi_small/\")\n",
    "data_dict = Seq2Seq_Dictionary(data)\n",
    "target_dict = Seq2Seq_Dictionary(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the data and the targets\n",
    "transformed_data = [data_dict.transform_sentence(i) for i in data]\n",
    "transformed_targets = [target_dict.transform_sentence(i) for i in targets]\n",
    "\n",
    "## Sanity check for the lengths of the data and the targets\n",
    "assert np.mean([len(x) for x in transformed_data])==data_dict.max_length+2\n",
    "assert np.mean([len(x) for x in transformed_targets])==target_dict.max_length+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. Length of the data: 15\n",
      "Max. Length of the target: 386\n",
      "Sample data: ['You', 'try', 'and', 'hold', 'me', 'down']\n",
      "Corresponding targets: ['9.0.4', '9.2', '9.0', '7.11.2', 'C5', '4.7.11', '4.7.11', '5.9.0', '2.5.9', 'C5', '4.8.11', '9.0.4', '9.2', '9.0', '7.11.2', 'C5']\n",
      "Sample transformed data: [1, 80, 166, 216, 94, 150, 57, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "Corresponding transformed targets: [1, 12, 53, 42, 47, 30, 48, 48, 39, 25, 30, 10, 12, 53, 42, 47, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max. Length of the data: {data_dict.max_length+2}\")\n",
    "print(f\"Max. Length of the target: {target_dict.max_length+2}\")\n",
    "print(f\"Sample data: {data[2]}\")\n",
    "print(f\"Corresponding targets: {targets[2]}\")\n",
    "print(f\"Sample transformed data: {transformed_data[2]}\")\n",
    "print(f\"Corresponding transformed targets: {transformed_targets[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "Tensorflow initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "START = 1\n",
    "END = 2\n",
    "\n",
    "embedding_size= 256\n",
    "hidden_units = 128\n",
    "keep_prob=0.5 # Dropout parameter\n",
    "batch_size = 64\n",
    "max_seq_length = data_dict.max_length\n",
    "vocab_size = len(data_dict.index2word_map)\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_encoder_inputs = tf.placeholder(shape=(batch_size, max_seq_length),\n",
    "                                 dtype=tf.int32, name='encoder_inputs')\n",
    "_encoder_seq_len = tf.placeholder(shape=(batch_size),\n",
    "                                 dtype=tf.int32, name='encoder_seq_lens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder part is created here. In the architecture, a bidirectional GRU cell is used after the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "_encoder_inputs = tf.placeholder(shape=(batch_size, max_seq_length),\n",
    "                                 dtype=tf.int32, name='encoder_inputs')\n",
    "_encoder_seq_len = tf.placeholder(shape=(batch_size),\n",
    "                                 dtype=tf.int32, name='encoder_seq_lens')\n",
    "### remove before here\n",
    "with tf.variable_scope(\"encoder\") as encoder_sc:\n",
    "    ## embeddings\n",
    "    enc_embed_var = tf.Variable(\n",
    "        tf.random_uniform([vocab_size,\n",
    "                           embedding_size],\n",
    "                          -1.0, 1.0), name='embedding')\n",
    "    \n",
    "    enc_embed = tf.nn.embedding_lookup(enc_embed_var, _encoder_inputs)\n",
    "    \n",
    "    # Forward direction cell\n",
    "    enc_gru_fw = tf.nn.rnn_cell.GRUCell(hidden_units)\n",
    "    # Backward direction cell\n",
    "    enc_gru_bw = tf.nn.rnn_cell.GRUCell(hidden_units)\n",
    "    \n",
    "    enc_dropout_fw = tf.contrib.rnn.DropoutWrapper(enc_gru_fw, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "\n",
    "    enc_dropout_bw = tf.contrib.rnn.DropoutWrapper(enc_gru_bw, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "\n",
    "    \n",
    "    ## here the state variable contains only the last state information of the cells\n",
    "    enc_rnn_outputs,enc_rnn_state=tf.nn.bidirectional_dynamic_rnn(enc_dropout_fw,\n",
    "                                                          enc_dropout_bw, \n",
    "                                                          enc_embed,\n",
    "                                                          sequence_length=_encoder_seq_len,\n",
    "                                                          dtype=tf.float32)\n",
    "    ## Get forward and backward last states and outputs of the GRU\n",
    "    enc_rnn_outputs_fw,enc_rnn_outputs_bw  = enc_rnn_outputs\n",
    "    enc_rnn_fw_state,enc_rnn_bw_state  = enc_rnn_state\n",
    "    \n",
    "    ## concat states and outputs\n",
    "    enc_last_state = tf.concat((enc_rnn_bw_state, enc_rnn_fw_state),1)\n",
    "    enc_output = tf.concat((enc_rnn_outputs_bw,enc_rnn_outputs_fw),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder part is created here. Because bidirectional GRU  is used in the encoder part the state vector is twice size of an GRU cell with same number of hidden units. So, after concatanating the last states of GRUs, here the hidden units of GRU should be doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc_last_state.get_shape())\n",
    "print(enc_output.get_shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_last_state = tf.placeholder(shape=(batch_size, 2*hidden_units),\n",
    "                                 dtype=tf.float32, name='decoder_input_enc_last_state')\n",
    "_enc_output = tf.placeholder(shape=(batch_size,max_seq_length ,2*hidden_units),\n",
    "                                 dtype=tf.float32, name='decoder_input_enc_last_state')\n",
    "_decoder_inputs = tf.placeholder(shape=(batch_size),\n",
    "                                 dtype=tf.int32, name='decoder_inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "_enc_last_state = tf.placeholder(shape=(batch_size, 2*hidden_units),\n",
    "                                 dtype=tf.float32, name='decoder_input_enc_last_state')\n",
    "_enc_output = tf.placeholder(shape=(batch_size,max_seq_length ,2*hidden_units),\n",
    "                                 dtype=tf.float32, name='decoder_input_enc_last_state')\n",
    "_decoder_inputs = tf.placeholder(shape=(batch_size),\n",
    "                                 dtype=tf.int32, name='decoder_inputs')\n",
    "with tf.variable_scope(\"decoder\") as decoder_sc:\n",
    "    ## Luong's multiplicative score --> score = hidden_state.T * W * _enc_output\n",
    "    \n",
    "    ### First the W*_enc_output part is handled. It is straightforward with a dense layer, \n",
    "    ### and its output size should be hidden_size*2, because we have a bidirectional rnn \n",
    "    ### in the encoder. Output shape should be (batch_size, max_len, 2*hidden_size)\n",
    "    ### because later it will be multiplied with (batch_size,2*hidden_size) (which could be thought\n",
    "    ### as batch_size, 2*hidden_size, 1) to get the score.\n",
    "    w_times_enc_output = tf.layers.dense(_enc_output, hidden_units*2)\n",
    "    print(\"shape of w_times_enc_output:\",w_times_enc_output.get_shape())\n",
    "    \n",
    "    ### \n",
    "    ### enc_last_state's size is (batch_size, 2*hidden_size) one can think of it as \n",
    "    ### (batch_size, 1,2*hidden_size). Semantically, there is only one hidden state vector\n",
    "    ### for each batch item(iteration).To transpose it, as the formula of Luong's suggests,\n",
    "    ### we can just expand (batch_size, 2*hidden_size) to (batch_size, 2*hidden_size,1), \n",
    "    ### expanding in the 2.nd dimension.\n",
    "    enc_last_state_tr = tf.expand_dims(_enc_last_state,2)\n",
    "    print(\"shape of enc_last_state_tr:\",enc_last_state_tr.get_shape())\n",
    "    \n",
    "    ### w_times_enc_output = (batch_size, max_len, 2*hidden_size)\n",
    "    ### enc_last_state_tr = (batch_size, 2*hidden_size,1)\n",
    "    ### resulting score = (batch_size, max_len,1)\n",
    "    score =  tf.matmul(w_times_enc_output,enc_last_state_tr)\n",
    "    print(\"shape of score:\",score.get_shape())\n",
    "    \n",
    "    ### Now the shape of score (batch_size, max_len,1). We have a score for each of the \n",
    "    ### input word in a bacth. To normalize it, now they are put in a softmax, and \n",
    "    ### the normalization should be within a batch, so the axis to apply softmax is\n",
    "    ### 1.st one, since 0 is used for batches.\n",
    "    ### Attention weights(attention_w) has same shape with score, which is (batch_size, max_len,1)\n",
    "    attention_w = tf.nn.softmax(score,1)\n",
    "    \n",
    "    ### attention_w (batch_size, max_len,1),   _enc_output (batch_size, max_len,2*hidden_size).\n",
    "    ### Multiplication operator supports broadcasting, so that this multiplication does not produce\n",
    "    ### an error. attention_w is broadcasted to be multiplied with each hidden unit of _enc_output,.\n",
    "    ### which means multiplying each output of the hidden units with the attention weight of the\n",
    "    ### associated word.\n",
    "    ### Resulting context_vec is in shape of (batch_size, max_len, 2*hidden_size)\n",
    "    context_vec = attention_w * _enc_output\n",
    "    \n",
    "    ### To create a context vector for each sentence in the batch, now we are summing\n",
    "    ### up along the dimension of the max_len(along words in a sentence) \n",
    "    ### so that we are left with size (batch_size, 2*hidden_size).\n",
    "    context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "    print(\"shape of context_vec:\",context_vec.get_shape())\n",
    "    \n",
    "    ### Input to the decoder is also put through a embedding layer, since they are\n",
    "    ### target sentences.\n",
    "    dec_embed_var = tf.Variable(\n",
    "        tf.random_uniform([vocab_size,\n",
    "                           embedding_size],\n",
    "                          -1.0, 1.0), name='decoder_embedding')\n",
    "    \n",
    "    ### Size of the embedded input-> (batch_size, 1, embedding_size)\n",
    "    dec_embed = tf.nn.embedding_lookup(dec_embed_var, tf.expand_dims(_decoder_inputs,1))\n",
    "    \n",
    "    print(\"shape of the decoder embedding:\",dec_embed.get_shape())\n",
    "    \n",
    "    ### To make the 1.st dimension matching with the embedded input, now the context vector \n",
    "    ### is expanded in the 1.st dimension. resulting size is (batch_size, 1, 2*hidden_size)\n",
    "    context_vec = tf.expand_dims(context_vec, 1)\n",
    "    \n",
    "    ### Concatanate along the second dimension, so the resulting size is\n",
    "    ### (batch_size, 1, 2*hidden_size + hidden_dim)\n",
    "    dec_before_gru = tf.concat([context_vec, dec_embed], axis=2)\n",
    "    \n",
    "    ### Since we will be feeding the decoder one input at a time, the sequence length\n",
    "    ### would be either 0 or 1 depending on the current input of each sentence.\n",
    "    ### So if the current input is not <PAD>, then the seq len is 1, if it is <PAD> then \n",
    "    ### it is just a padding, the seq len is 0.\n",
    "    all_pads = [data_dict.get_index(\"<PAD>\")]*batch_size\n",
    "    ones = np.ones((batch_size))\n",
    "    zeros = np.zeros((batch_size))\n",
    "    dec_seq_len = tf.cast(tf.where(_decoder_inputs == all_pads, zeros, ones),dtype=tf.float32)\n",
    "    \n",
    "    ### Now the input is ready for the GRU.\n",
    "    dec_gru = tf.nn.rnn_cell.GRUCell(2*hidden_units)\n",
    "    \n",
    "    dec_dropout = tf.contrib.rnn.DropoutWrapper(dec_gru, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "    \n",
    "    ### dec_rnn_outputs has shape (batch_size, 1, 2*hidden_size)\n",
    "    ### dec_rnn_state has shape (batch_size, 2*hidden_size)\n",
    "    dec_rnn_outputs,dec_rnn_state=tf.nn.dynamic_rnn(cell=dec_dropout, inputs=dec_before_gru, \n",
    "                                                    initial_state=_enc_last_state,\n",
    "                                                    sequence_length=dec_seq_len)\n",
    "    ### To make predictions based on the output of the rnn, now we are reshaping the \n",
    "    ### the output to the shape of (batch_size, 2*hidden_size)\n",
    "    dec_rnn_outputs = tf.squeeze(dec_rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"pred_layer\") as pred_layer_sc:\n",
    "    ### predictions has the shape of (batch_size, vocab_size). This means we are predicting\n",
    "    ### only the next word for each sentence. For each sentence, there is a vector of\n",
    "    ### shape vocab_size which contains the likelihood of the corresponding vocabulary\n",
    "    ### element for the next word in the sentence.\n",
    "    preds = tf.layers.dense(dec_rnn_outputs, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dec_rnn_outputs.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and the Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_targets = tf.placeholder(shape=(batch_size,vocab_size), \n",
    "                                 dtype=tf.int32, name='Targets')\n",
    "\n",
    "### If the input word is <PAD>, then there is no need for optimization for that input.\n",
    "cross_ent = tf.nn.softmax_cross_entropy_with_logits_v2(logits=preds, labels=_targets) * dec_seq_len\n",
    "\n",
    "### mean of the cross entropy is the loss of this batch\n",
    "loss = tf.reduce_mean(cross_ent)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
