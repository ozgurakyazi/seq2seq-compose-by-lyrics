{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Through Lyrics\n",
    "\n",
    "The purpose of this project is to learn the relation between the lyrics of a song and its music. It is certain that different kind of lyrics require some different kind of music that indicate the connotation behind the used words. From a technical point of view, we have two sequences that we are trying to relate:\n",
    "\n",
    "1) The first sequence is a list of words of the lyrics of the song.\n",
    "\n",
    "2) The second sequence is a list of notes/chords that compose the music of the song.\n",
    "\n",
    "With that being said, we built an encoder/decoder seq2seq model with GRU recurrent units to try to capture the relationship between the lyrics and the music.\n",
    "\n",
    "In order to aquire some training data we needed to get creative. We thought of gathering midi files of songs that already contain the lyrics and use these as training data. Although the idea is valid, but gathering the data was a huge challenge towards this project. Most of the midi files on the internet is either expensive to get (it is the same kind of files that is used for kareoke), or the files are assumed that they contain the lyrics while in reality they do not. Luckily we were able to find http://www.olgris.kiev.ua/des/midi%20lat.html, which was our main source of data. The website contains 230 midi files with all different genres of music.\n",
    "\n",
    "## Background Knowledge\n",
    "In order to be able to work with the midi files, we needed to know what a midi file is in the first place in order to look in the right place for the data required. Without going into much detail, midi files are byte-encoded files that include all different information about a piece of music. The most important building units of a midi file are the tracks. A midi file includes a track for every instrument played in that song. However there is one special track, usually that of the piano, that includes also the lyrics. Each track consists of several events of different types. Each event contains then its own metadata that is used when playing the file. There are two important types of events in every track:\n",
    "\n",
    "1) The \"NOTE\" event which is translated to a certain sound, and\n",
    "    \n",
    "2) The \"LYRIC\" event that contains the word or words at a specific position.\n",
    "    \n",
    "Events in a track are sequential data and are thus played in certain order. This is why the piano track is considered for this project. All notes/chords sequence associated with a sentence is extracted and treated as a training sample.\n",
    "\n",
    "As mentioned above, midi files are byte-encoded files, and it would have been quite a hassle to work with them as raw files. This is why we used the music21 library developed at MIT https://web.mit.edu/music21/. Before getting started, please follow the installation instructions stated below.\n",
    "\n",
    "## Installation steps\n",
    "Please install all packages listed in requirements.txt by running \"pip install -r requirements.txt\"\n",
    "In addition to that, spaCy was used for text preprocessing. This is why a spaCy model needs to be downloaded with the command \"python -m spacy download en\". Ultimately, the package timidity needs to be installed in order to play music in this notebook. Install timidity via the command \"sudo apt-get install timidity timidity-interfaces-extra\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A view on the data\n",
    "Thanks to music21 and timidity, we can play midi files from a notebook. Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preprocess import *\n",
    "file = \"test_midi/14_Years.mid\"\n",
    "play_midi(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lyrics of the song is included also in the midi file. If we were to play it in a terminal using \"timidity -filename-\", we would get the lyrics as in a kareoke game. Because the piano track is the only track of interest to us, we need to look at the events in order to know the type of the data we are dealing with. Here are 10 events from that track: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = midi.MidiFile()\n",
    "m.open(file)\n",
    "m.read()\n",
    "for track in m.tracks:\n",
    "    lyrics = [ev.data for ev in track.events if ev.type==\"LYRIC\"]\n",
    "    temp_stream = midi.translate.midiTrackToStream(track)\n",
    "    notes = get_notes_from_stream(temp_stream)\n",
    "    if len(lyrics) > 0:\n",
    "        break\n",
    "print(track.events[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these events, we came up with a string representation that would allow us to convert the strings back to notes/chords. The string representation of 20 notes/chords from this track are shown below. As we can see, we convert notes by mapping them to their pitches. As for chords, a string of the normal order representation of each note in the chord separated by a dot is recorded. For the detailed operation, please refer to the \"get_notes_from_stream\" in preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notes[110:130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the data is ready and can be fed to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains special words for the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_words = [\"<PAD>\", \"<GO>\", \"<END>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class For Dictionary\n",
    "The class contains the data and the helper functions for the dictionary to train seq2seq model.\n",
    "Most important functions are to mapping a list of words(a sentence) to a list of corresponding integer(indeces of words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Dictionary:\n",
    "    def __init__(self, sentences ):\n",
    "        self.word2index_map = dict()\n",
    "        self.index2word_map = dict()\n",
    "        self.vocab_size = 0\n",
    "        self.init_register(sentences)\n",
    "        \n",
    "    # Initiates word2index_map and index2word_map\n",
    "    # Also extracts the max number of words in the sentences and saves it \n",
    "    def init_register(self,sentences):\n",
    "        global special_words\n",
    "        current_index = 0\n",
    "        ## save the maximum length among the sentences. \n",
    "        self.max_length = max([len(sentence) for sentence in sentences])\n",
    "        ### map special words, initially the mappings are empty.\n",
    "        for word in special_words:\n",
    "            self.word2index_map[word] = current_index\n",
    "            self.index2word_map[current_index] = word\n",
    "            current_index+=1\n",
    "        \n",
    "        s = set([item for sublist in sentences for item in sublist])\n",
    "        self.word2index_map.update({e:i+current_index for i,e in enumerate(s)})\n",
    "        self.index2word_map.update({v:k for k,v in self.word2index_map.items()})\n",
    "        self.vocab_size = len(self.index2word_map)\n",
    "    \n",
    "    ## Returns the index of the word in the dictionary. It is assumed that the word\n",
    "    ## will be always in dictionary.\n",
    "    def get_index(self, word):\n",
    "        return self.word2index_map[word]\n",
    "    \n",
    "    ## Maps a sentence, which is a list of words, to the corresponding list of integers.\n",
    "    ## Each word is looked up from the map of the dictionary, and as in get_index method,\n",
    "    ## it is assumed that the word will always be found in the dictionary\n",
    "    def map_sentence(self, sentence):\n",
    "        return [self.get_index(i) if i in self.word2index_map else 0 for i in sentence]\n",
    "    \n",
    "    ## Returns the word by its index in dictionary.\n",
    "    def get_word(self, index):\n",
    "        return self.index2word_map[word]\n",
    "    \n",
    "    ## Pads the list of words to <PAD> at the end of the list of words in sentence\n",
    "    def pad_sentence(self, sentence):\n",
    "        return  sentence + [\"<PAD>\"] * (self.max_length - len(sentence)+2)\n",
    "    \n",
    "    ## Adds <GO> and <END> to the start and end of the sentence\n",
    "    def add_start_end_tokens(self, sentence):\n",
    "        return [\"<GO>\"] + sentence + [\"<END>\"]\n",
    "    \n",
    "    ## Transforms the sentence in a format suitable for the Neural Network\n",
    "    def transform_sentence(self, sentence):\n",
    "        s = self.add_start_end_tokens(sentence)\n",
    "        s = self.pad_sentence(s)\n",
    "        s = self.map_sentence(s)\n",
    "        return s\n",
    "    \n",
    "    def reverse_transform(self, sentence):\n",
    "        return [self.index2word_map[s] for s in sentence if (not self.index2word_map[s] in special_words and s in self.index2word_map)]\n",
    "    \n",
    "    ### Reverse operation of map_sentence. Gets the sentence, list of integers, as input and\n",
    "    ### maps each entry from index to word\n",
    "    #def decode_indeces(self,sentence_with_index):\n",
    "    #    sentence_list = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = get_data_from_dir(\"test_midi_small/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = Seq2Seq_Dictionary(data)\n",
    "target_dict = Seq2Seq_Dictionary(targets)\n",
    "max_sentence_len = data_dict.max_length\n",
    "max_notes_len = target_dict.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the data and the targets\n",
    "transformed_data = [data_dict.transform_sentence(i) for i in data]\n",
    "transformed_targets = [target_dict.transform_sentence(i) for i in targets]\n",
    "print(data_dict.max_length)\n",
    "print(data[10])\n",
    "print(transformed_data[10])\n",
    "## Sanity check for the lengths of the data and the targets\n",
    "# assert np.mean([len(x) for x in transformed_data])==data_dict.max_length\n",
    "# assert np.mean([len(x) for x in transformed_targets])==target_dict.max_length\n",
    "# assert len(transformed_data) == len(transformed_targets)\n",
    "\n",
    "len(data_dict.index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of data vocabulary: {data_dict.vocab_size}\")\n",
    "print(f\"Size of targets vocabulary: {target_dict.vocab_size}\")\n",
    "\n",
    "print(f\"Max. Length of the data: {data_dict.max_length}\")\n",
    "print(f\"Max. Length of the target: {target_dict.max_length}\")\n",
    "print(f\"Sample data: {data[10]}\")\n",
    "print(f\"Corresponding targets: {targets[2]}\")\n",
    "print(f\"Sample transformed data: {transformed_data[10]}\")\n",
    "print(f\"Corresponding transformed targets: {transformed_targets[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "Tensorflow initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size= 256\n",
    "hidden_units = 128\n",
    "keep_prob=0.5 # Dropout parameter\n",
    "batch_size = 32\n",
    "sentence_vocab_size = len(data_dict.index2word_map)\n",
    "notes_vocab_size = len(target_dict.index2word_map)\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_encoder_inputs = tf.placeholder(shape=(batch_size, max_sentence_len+2),\n",
    "                                 dtype=tf.int32, name='encoder_inputs')\n",
    "_encoder_seq_len = tf.placeholder(shape=(batch_size),\n",
    "                                 dtype=tf.int32, name='encoder_seq_lens')\n",
    "_is_training = tf.placeholder(tf.bool,name=\"training_or_test\")\n",
    "_target_notes = tf.placeholder(shape=(batch_size, max_notes_len+2) , \n",
    "                               dtype=tf.int32, name='target_notes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder part is created here. In the architecture, a bidirectional GRU cell is used after the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _encoder_inputs = tf.placeholder(shape=(batch_size, max_sentence_len),\n",
    "#                                  dtype=tf.int32, name='encoder_inputs')\n",
    "# _encoder_seq_len = tf.placeholder(shape=(batch_size),\n",
    "#                                  dtype=tf.int32, name='encoder_seq_lens')\n",
    "# _is_training = tf.placeholder(tf.bool,name=\"training_or_test\")\n",
    "# _target_notes = tf.placeholder(shape=(batch_size, max_notes_len) , \n",
    "#                                dtype=tf.int32, name='target_notes')\n",
    "### remove before here\n",
    "with tf.variable_scope(\"encoder\") as encoder_sc:\n",
    "    ## embeddings\n",
    "    enc_embed_var = tf.Variable(\n",
    "        tf.random_uniform([sentence_vocab_size,\n",
    "                           embedding_size],\n",
    "                          -1.0, 1.0), name='embedding')\n",
    "    \n",
    "    enc_embed = tf.nn.embedding_lookup(enc_embed_var, _encoder_inputs)\n",
    "    \n",
    "    # Forward direction cell\n",
    "    enc_gru_fw = tf.nn.rnn_cell.GRUCell(hidden_units)\n",
    "    # Backward direction cell\n",
    "    enc_gru_bw = tf.nn.rnn_cell.GRUCell(hidden_units)\n",
    "    \n",
    "    enc_dropout_fw = tf.contrib.rnn.DropoutWrapper(enc_gru_fw, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "\n",
    "    enc_dropout_bw = tf.contrib.rnn.DropoutWrapper(enc_gru_bw, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "\n",
    "    \n",
    "    ## here the state variable contains only the last state information of the cells\n",
    "    enc_rnn_outputs,enc_rnn_state=tf.nn.bidirectional_dynamic_rnn(enc_dropout_fw,\n",
    "                                                          enc_dropout_bw, \n",
    "                                                          enc_embed,\n",
    "                                                          sequence_length=_encoder_seq_len,\n",
    "                                                          dtype=tf.float32)\n",
    "    ## Get forward and backward last states and outputs of the GRU\n",
    "    enc_rnn_outputs_fw,enc_rnn_outputs_bw  = enc_rnn_outputs\n",
    "    enc_rnn_fw_state,enc_rnn_bw_state  = enc_rnn_state\n",
    "    \n",
    "    ## concat states and outputs\n",
    "    _enc_last_state = tf.concat((enc_rnn_bw_state, enc_rnn_fw_state),1)\n",
    "    _enc_output = tf.concat((enc_rnn_outputs_bw,enc_rnn_outputs_fw),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_enc_last_state.get_shape())\n",
    "print(_enc_output.get_shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder part is created here. Because bidirectional GRU  is used in the encoder part the state vector is twice size of an GRU cell with same number of hidden units. So, after concatanating the last states of GRUs, here the hidden units of GRU should be doubled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder  With While Loop\n",
    "We are using a while loop structure because each resulting hidden state of the GRU in the decoder, will be an input to the network to calculate scores of the next word in the same sentence.\n",
    "\n",
    "Following is the condition for the while loop. From the first word at each sentence, iteration should go until the last word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decoder_condition(t, *args):\n",
    "    return t<max_notes_len+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder as a function to be called from the body of the while_loop. Note that, in order to reuse the network after each word, first we need to initialize it then set the reuse to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#_enc_last_state = tf.placeholder(shape=(batch_size, 2*hidden_units),\n",
    "#                                 dtype=tf.float32, name='decoder_input_enc_last_state')\n",
    "#_enc_output = tf.placeholder(shape=(batch_size,max_seq_length ,2*hidden_units),\n",
    "#                                 dtype=tf.float32, name='decoder_input_enc_last_state')\n",
    "#_decoder_inputs = tf.placeholder(shape=(batch_size),\n",
    "#                                 dtype=tf.int32, name='decoder_inputs')\n",
    "\n",
    "def decoder(_decoder_inputs,_hidden_state,reuse=None):\n",
    "    with tf.variable_scope(\"decoder\",reuse=reuse) as decoder_sc:\n",
    "        ## Luong's multiplicative score --> score = _hidden_state.T * W * _enc_output\n",
    "\n",
    "        ### First the W*_enc_output part is handled. It is straightforward with a dense layer, \n",
    "        ### and its output size should be hidden_size*2, because we have a bidirectional rnn \n",
    "        ### in the encoder. Output shape should be (batch_size, max_len, 2*hidden_size)\n",
    "        ### because later it will be multiplied with (batch_size,2*hidden_size) (which could be thought\n",
    "        ### as batch_size, 2*hidden_size, 1) to get the score.\n",
    "        w_times_enc_output = tf.layers.dense(_enc_output, hidden_units*2)\n",
    "        print(\"shape of w_times_enc_output:\",w_times_enc_output.get_shape())\n",
    "\n",
    "        ### First hidden state is taken from the encoder's GRUs last hidden state. So the\n",
    "        ### shape of it is (batch_size, 2*hidden_size). For each input sentence, there is one\n",
    "        ### hidden state.\n",
    "        ### _hidden_state's size is (batch_size, 2*hidden_size) one can think of it as \n",
    "        ### (batch_size, 1,2*hidden_size). Semantically, there is only one hidden state vector\n",
    "        ### for each batch item(iteration).To transpose it, as the formula of Luong's suggests,\n",
    "        ### we can just expand (batch_size, 2*hidden_size) to (batch_size, 2*hidden_size,1), \n",
    "        ### expanding in the 2.nd dimension.\n",
    "        hidden_state_tr = tf.expand_dims(_hidden_state,2)\n",
    "        print(\"shape of enc_last_state_tr:\",hidden_state_tr.get_shape())\n",
    "\n",
    "        ### w_times_enc_output = (batch_size, max_len, 2*hidden_size)\n",
    "        ### enc_last_state_tr = (batch_size, 2*hidden_size,1)\n",
    "        ### resulting score = (batch_size, max_len,1)\n",
    "        score =  tf.matmul(w_times_enc_output,hidden_state_tr)\n",
    "        print(\"shape of score:\",score.get_shape())\n",
    "\n",
    "        ### Now the shape of score (batch_size, max_len,1). We have a score for each of the \n",
    "        ### input word in a bacth. To normalize it, now they are put in a softmax, and \n",
    "        ### the normalization should be within a batch, so the axis to apply softmax is\n",
    "        ### 1.st one, since 0 is used for batches.\n",
    "        ### Attention weights(attention_w) has same shape with score, which is (batch_size, max_len,1)\n",
    "        attention_w = tf.nn.softmax(score,1)\n",
    "\n",
    "        ### attention_w (batch_size, max_len,1),   _enc_output (batch_size, max_len,2*hidden_size).\n",
    "        ### Multiplication operator supports broadcasting, so that this multiplication does not produce\n",
    "        ### an error. attention_w is broadcasted to be multiplied with each hidden unit of _enc_output,.\n",
    "        ### which means multiplying each output of the hidden units with the attention weight of the\n",
    "        ### associated word.\n",
    "        ### Resulting context_vec is in shape of (batch_size, max_len, 2*hidden_size)\n",
    "        context_vec = attention_w * _enc_output\n",
    "\n",
    "        ### To create a context vector for each sentence in the batch, now we are summing\n",
    "        ### up along the dimension of the max_len(along words in a sentence) \n",
    "        ### so that we are left with size (batch_size, 2*hidden_size).\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "        print(\"shape of context_vec:\",context_vec.get_shape())\n",
    "\n",
    "        ### Input to the decoder is also put through a embedding layer, since they are\n",
    "        ### target sentences.\n",
    "        embed_var = lambda: tf.random_uniform([notes_vocab_size,embedding_size],-1.0, 1.0)\n",
    "        dec_embed_var = tf.Variable(embed_var ,name='decoder_embedding')\n",
    "\n",
    "        ### Size of the embedded input-> (batch_size, 1, embedding_size)\n",
    "        dec_embed = tf.nn.embedding_lookup(dec_embed_var, tf.expand_dims(_decoder_inputs,1))\n",
    "\n",
    "        print(\"shape of the decoder embedding:\",dec_embed.get_shape())\n",
    "\n",
    "        ### To make the 1.st dimension matching with the embedded input, now the context vector \n",
    "        ### is expanded in the 1.st dimension. resulting size is (batch_size, 1, 2*hidden_size)\n",
    "        context_vec = tf.expand_dims(context_vec, 1)\n",
    "\n",
    "        ### Concatanate along the second dimension, so the resulting size is\n",
    "        ### (batch_size, 1, 2*hidden_size + hidden_dim)\n",
    "        dec_before_gru = tf.concat([context_vec, dec_embed], axis=2)\n",
    "\n",
    "        ### Since we will be feeding the decoder one input at a time, the sequence length\n",
    "        ### would be either 0 or 1 depending on the current input of each sentence.\n",
    "        ### So if the current input is not <PAD>, then the seq len is 1, if it is <PAD> then \n",
    "        ### it is just a padding, the seq len is 0.\n",
    "        all_pads = [target_dict.get_index(\"<PAD>\")]*batch_size\n",
    "        ones = np.ones((batch_size))\n",
    "        zeros = np.zeros((batch_size))\n",
    "        dec_seq_len = tf.cast(tf.where(_decoder_inputs == all_pads, zeros, ones),\n",
    "                              dtype=tf.float32)\n",
    "\n",
    "        ### Now the input is ready for the GRU.\n",
    "        dec_gru = tf.nn.rnn_cell.GRUCell(2*hidden_units)\n",
    "\n",
    "        dec_dropout = tf.contrib.rnn.DropoutWrapper(dec_gru, input_keep_prob=keep_prob,\n",
    "                                                       output_keep_prob=keep_prob)\n",
    "\n",
    "        ### dec_rnn_outputs has shape (batch_size, 1, 2*hidden_size)\n",
    "        ### dec_rnn_state has shape (batch_size, 2*hidden_size)\n",
    "        dec_rnn_outputs,dec_rnn_state=tf.nn.dynamic_rnn(cell=dec_dropout, inputs=dec_before_gru, \n",
    "                                                        initial_state=_hidden_state,\n",
    "                                                        sequence_length=dec_seq_len)\n",
    "        ### To make predictions based on the output of the rnn, now we are reshaping the \n",
    "        ### the output to the shape of (batch_size, 2*hidden_size)\n",
    "        dec_rnn_outputs = tf.squeeze(dec_rnn_outputs)\n",
    "\n",
    "        ### predictions has the shape of (batch_size, notes_vocab_size). This means we are predicting\n",
    "        ### only the next word for each sentence. For each sentence, there is a vector of\n",
    "        ### shape notes_vocab_size which contains the likelihood of the corresponding vocabulary\n",
    "        ### element for the next word in the sentence.\n",
    "        preds = tf.layers.dense(dec_rnn_outputs, notes_vocab_size)\n",
    "    \n",
    "    return preds, dec_rnn_state,dec_seq_len  #,dec_embed_var.read_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.variable_scope(\"pred_layer\") as pred_layer_sc:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the decoder inputs. \n",
    "To reduce the memory usage during the backpropagation, we are putting the each word in each sentences in a batch into a TensorArray.\n",
    "\n",
    "The size of the decoder inputs is the (batch_size, max_notes_len), aproximately the target notes of the current input batch. First we are transposing it to word major order, so that the [i,] indexing will return the i.th note of the all note sequences in the batch. Then we are going to unstack it, hence we will get a tensorarray of size max_notes_len, each item containing batch_size notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General variables to run the decoder loop. \n",
    "\n",
    "Iteration starts from 1, because we are going to call the decoder to initialize it at the beginning. \n",
    "\n",
    "init_outputs stores the output predictions of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_i = tf.constant(1, dtype=tf.int32)\n",
    "init_outputs = tf.TensorArray(dtype=tf.float32,size=max_notes_len+2,clear_after_read=False)\n",
    "init_seq_len = tf.TensorArray(dtype=tf.float32,size=max_notes_len+2,clear_after_read=False)\n",
    "#init_embed_vals = tf.random_uniform([vocab_size,embedding_size],-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the decoder to be able to \"reuse\" it, note that reuse is None as default. Initial hidden state is from the encoder and first decoder input is the 0.th element of the decoder_input_arr. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_preds,init_hidden_state,temp_seq_len = decoder(\n",
    "                                        [target_dict.get_index(\"<GO>\")]*(batch_size), \n",
    "                                        _enc_last_state)\n",
    "                                       #init_embed_vals)\n",
    "init_outputs = init_outputs.write(0, init_preds)\n",
    "init_seq_len = init_seq_len.write(0, temp_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the decoder with the while loop. If we are using the teacher forcing, we need to use this function. But in the test time, we cannot use it, so we should be using  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_body_teacher_forcing(iteration,outputs,body_hidden_state,seq_len,decoder_input_arr):\n",
    "    temp_preds,temp_hid_state,temp_seq_len = decoder(\n",
    "                                    _decoder_inputs=decoder_input_arr.read(iteration), \n",
    "                                    _hidden_state=body_hidden_state,\n",
    "                                    #_embedding_var = embed_val,\n",
    "                                    reuse=True)\n",
    "    outputs = outputs.write(iteration, temp_preds)\n",
    "    seq_len = seq_len.write(iteration, temp_seq_len)\n",
    "    return iteration+1, outputs, temp_hid_state,seq_len,decoder_input_arr ##,temp_embed_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_body_test(iteration,outputs,body_hidden_state,seq_len,fake_decoder_input):\n",
    "    print(\"before argmax\")\n",
    "    print(tf.argmax(outputs.read(iteration-1), axis=1).get_shape())\n",
    "    print(\"after argmax\")\n",
    "    temp_preds,temp_hid_state,temp_seq_len = decoder(\n",
    "                                    _decoder_inputs=tf.argmax(outputs.read(iteration-1), axis=1), \n",
    "                                    _hidden_state=body_hidden_state,\n",
    "                                    #_embedding_var = embed_val,\n",
    "                                    reuse=True)\n",
    "    outputs = outputs.write(iteration, temp_preds)\n",
    "    seq_len = seq_len.write(iteration, temp_seq_len)\n",
    "    return iteration+1, outputs, temp_hid_state,seq_len,fake_decoder_input       ##,temp_embed_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally run the while loop. We dont need the latest hidden state and the iteration count, the only thing needed is the predictions of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_while_teacher_f():\n",
    "    decoder_inputs_tr = tf.transpose(_target_notes)\n",
    "    decoder_input_arr = tf.TensorArray(dtype=tf.int32, size=max_notes_len+2,clear_after_read=False)\n",
    "    decoder_input_arr = decoder_input_arr.unstack(decoder_inputs_tr)\n",
    "    return tf.while_loop(decoder_condition, decoder_body_teacher_forcing, \n",
    "                    [init_i, init_outputs, init_hidden_state,init_seq_len,decoder_input_arr])\n",
    "def decoder_while_test():\n",
    "    return tf.while_loop(decoder_condition, decoder_body_test, \n",
    "                                [init_i, init_outputs, init_hidden_state,init_seq_len,1.0])\n",
    "_, dec_preds, _ ,seq_len,_= tf.cond(pred=_is_training, \n",
    "                                  true_fn=decoder_while_teacher_f,\n",
    "                                  false_fn=decoder_while_test\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, dec_preds contains the predictions for each word. So each item in the tensorarray contains (batch_size, vocab_size) shaped tensors. \n",
    "\n",
    "Now we are going to create a one tensor whose shape is (batch, max_len,vocab_size). To achieve it, we are going to stack the dec_preds, which results in (max_notes_len, batch_size, vocab_size). Then transpose to get (batch_size,max_notes_len,vocab_size). And the same is done for the seq_len, except the we want seq_len as shape of (batch_size, max_notes_len). \n",
    "\n",
    "The all_seq_len variable will be used as a mask to the input. Because if it is 0, then the word is a <PAD>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.transpose(dec_preds.stack(), [1,0,2])\n",
    "all_seq_len = tf.to_float(tf.transpose(seq_len.stack()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and the Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the mask for the backpropagation. If the target is <PAD> then don't backpropagate. To do it, first get all <PAD> strings as 1, and then subtract from 1 to make them all 0 s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If the input word is <PAD>, then there is no need for optimization for that input.\n",
    "#target_one_hot = tf.one_hot(indices=_target_notes, depth=notes_vocab_size)\n",
    "print(preds.shape, _target_notes.shape)\n",
    "cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds, labels=_target_notes) * all_seq_len\n",
    "\n",
    "### mean of the cross entropy is the loss of this batch\n",
    "loss = tf.reduce_mean(cross_ent)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step = optimizer.minimize(cross_ent)\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "def generate_batches(x, y, batch_size):\n",
    "    assert len(x) == len(y)\n",
    "    shuffeled_pairs = list(zip(x,y))\n",
    "    np.random.shuffle(shuffeled_pairs)\n",
    "    x = [i[0] for i in shuffeled_pairs]\n",
    "    y = [i[1] for i in shuffeled_pairs]\n",
    "    res = []\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        x_batch = x[i:min(i + batch_size, len(x))] \n",
    "        y_batch = y[i:min(i + batch_size, len(y))]\n",
    "        if len(x_batch) % batch_size == 0:\n",
    "            res.append({\n",
    "                _encoder_inputs: x_batch,\n",
    "                _encoder_seq_len: [sum(i > 2 for i in seq) for seq in x_batch],\n",
    "                _target_notes: y_batch,\n",
    "                _is_training:True\n",
    "            })\n",
    "    return res\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "epochs = 10\n",
    "loss_track = []\n",
    "evaluate_every = 8\n",
    "for epoch in range(epochs):\n",
    "    batches = generate_batches(transformed_data, transformed_targets, batch_size)\n",
    "    if epoch == 0:\n",
    "        print(f\"Number of batches: {len(batches)}\")\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for e, batch in enumerate(batches) :\n",
    "        feed_dict = batch\n",
    "        _, l = sess.run([train_step, loss], feed_dict)\n",
    "        loss_track.append(l)\n",
    "    print(f\"Last batch loss: {l}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "def compose(lyrics):\n",
    "    lyrics_list, mask = prepare_prediction(lyrics, batch_size, data_dict)\n",
    "    pred_ = sess.run([preds],\n",
    "        feed_dict={\n",
    "            _encoder_inputs: lyrics_list,\n",
    "            _encoder_seq_len: [sum(i > 2 for i in seq) for seq in lyrics_list],\n",
    "            _is_training:False,\n",
    "            _target_notes:np.ones((batch_size,target_dict.max_length+2))\n",
    "        })\n",
    "    pred = np.argmax(pred_[0], axis=2)\n",
    "    pred = [target_dict.reverse_transform(i) for i in pred]\n",
    "    return pred, mask\n",
    "    \n",
    "lyrics = [\"sad\",\"happy\",\"i whine like a baby\",\"i want to love, laugh and smile\"]\n",
    "\n",
    "pred, mask = compose(lyrics)\n",
    "for e, p in enumerate(pred):\n",
    "    if e < batch_size-mask:\n",
    "        print(lyrics[e])\n",
    "        fname = f\"sample_{lyrics[e]}\"\n",
    "        notes_to_midi(p, fname)\n",
    "        play_midi(f\"output/{fname}.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
