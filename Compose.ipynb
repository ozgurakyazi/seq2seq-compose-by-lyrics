{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preprocess import play_midi, parse_file,get_data_from_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains special words for the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_words = [\"<PAD>\", \"<GO>\", \"<END>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class For Dictionary\n",
    "The class contains the data and the helper functions for the dictionary to train seq2seq model.\n",
    "Most important functions are to mapping a list of words(a sentence) to a list of corresponding integer(indeces of words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Dictionary:\n",
    "    def __init__(self, sentences ):\n",
    "        self.word2index_map = dict()\n",
    "        self.index2word_map = dict()\n",
    "        self.vocab_size = 0\n",
    "        self.init_register(sentences)\n",
    "        \n",
    "    # Initiates word2index_map and index2word_map\n",
    "    # Also extracts the max number of words in the sentences and saves it \n",
    "    def init_register(self,sentences):\n",
    "        global special_words\n",
    "        current_index = 0\n",
    "        ## save the maximum length among the sentences. \n",
    "        self.max_length = max([len(sentence) for sentence in sentences])+2\n",
    "        ### map special words, initially the mappings are empty.\n",
    "        for word in special_words:\n",
    "            self.word2index_map[word] = current_index\n",
    "            self.index2word_map[current_index] = word\n",
    "            current_index+=1\n",
    "        \n",
    "        s = set([item for sublist in sentences for item in sublist])\n",
    "        self.word2index_map.update({e:i+current_index for i,e in enumerate(s)})\n",
    "        self.index2word_map.update({v:k for k,v in self.word2index_map.items()})\n",
    "        self.vocab_size = len(self.index2word_map)\n",
    "    \n",
    "    ## Returns the index of the word in the dictionary. It is assumed that the word\n",
    "    ## will be always in dictionary.\n",
    "    def get_index(self, word):\n",
    "        return self.word2index_map[word]\n",
    "    \n",
    "    ## Maps a sentence, which is a list of words, to the corresponding list of integers.\n",
    "    ## Each word is looked up from the map of the dictionary, and as in get_index method,\n",
    "    ## it is assumed that the word will always be found in the dictionary\n",
    "    def map_sentence(self, sentence):\n",
    "        return [self.get_index(i) for i in sentence if i in self.word2index_map]\n",
    "    \n",
    "    ## Returns the word by its index in dictionary.\n",
    "    def get_word(self, index):\n",
    "        return self.index2word_map[word]\n",
    "    \n",
    "    ## Pads the list of words to <PAD> at the end of the list of words in sentence\n",
    "    def pad_sentence(self, sentence):\n",
    "        return [\"<PAD>\"] * (self.max_length - len(sentence)-2)+sentence \n",
    "    \n",
    "    ## Adds <GO> and <END> to the start and end of the sentence\n",
    "    def add_start_end_tokens(self, sentence):\n",
    "        return [\"<GO>\"] + sentence + [\"<END>\"]\n",
    "    \n",
    "    ## Transforms the sentence in a format suitable for the Neural Network\n",
    "    def transform_sentence(self, sentence):\n",
    "        s = self.pad_sentence(sentence)\n",
    "        s = self.add_start_end_tokens(s)\n",
    "        s = self.map_sentence(s)\n",
    "        return s\n",
    "    \n",
    "    ### Reverse operation of map_sentence. Gets the sentence, list of integers, as input and\n",
    "    ### maps each entry from index to word\n",
    "    #def decode_indeces(self,sentence_with_index):\n",
    "    #    sentence_list = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For file test_midi_small/All That She Wants.mid extracted:\n",
      "Text: [b'She ', b'leads ', b'a ', b'lone', b'ly ', b'life ', b'\\r', b'Ooh ', b'She ', b'leads ', b'a ', b'lone', b'ly ', b'life ', b'\\r', b'When ', b'she ', b'woke ', b'up ', b'late ', b'in ', b'the ', b'mor', b'ning ', b'\\r', b'Light ', b'and ', b'the ', b'day ', b'had ', b'just ', b'be', b'gu', b'n ', b'\\r', b'She ', b'o', b'pened ', b'up ', b'her ', b'eyes ', b'and_thought ', b'\\r', b\"O' \", b'what ', b'a ', b'morn', b'ing ', b'\\r', b\"It's \", b'not ', b'a ', b'day ', b'for ', b'wor', b'k ', b'\\r', b\"It's \", b'a ', b'day ', b'for ', b'cat', b'ching ', b'tan ', b'\\r', b'Just ', b'la', b'ying ', b'on ', b'the ', b'beach ', b'and ', b'ha', b'ving ', b'fun ', b'\\r', b\"She's \", b'go', b'ing ', b'to ', b'get_you ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is ', b'a', b'no', b'ther ', b'ba', b'by ', b'\\r', b\"She's \", b'gone ', b'to', b'mo', b'rrow ', b'boy ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is ', b'a', b'no', b'ther ', b'ba', b'by ', b'a', b'yea ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is ', b'a', b'no', b'ther ', b'ba', b'by ', b'\\r', b\"She's \", b'gone ', b'to', b'mo', b'rrow ', b'boy ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is ', b'a', b'no', b'ther ', b'ba', b'by ', b'a', b'aa ', b'\\r', b'[All ', b'that ', b'she ', b'wants ', b'all ', b'that ', b'she ', b'wants] ', b'\\r', b'So ', b'if ', b'you ', b'are ', b'in ', b'sight ', b'and ', b'the ', b'day ', b'is ', b'right ', b'\\r', b\"She's \", b'a ', b'hun', b'ter ', b\"you're \", b'the ', b'fo', b'x ', b'\\r', b'The ', b'gen', b'tle ', b'voice ', b'that ', b'talks ', b'to_you ', b'\\r', b\"Won't \", b'talk ', b'for', b'e', b'ver ', b'\\r', b'It ', b'is ', b'a ', b'night ', b'for ', b'pa', b'ssion ', b'\\r', b'But ', b'the ', b'mor', b'ning ', b'means ', b'good', b'bye ', b'\\r', b'Be', b'ware ', b'of ', b'what ', b'is ', b'fla', b'shing ', b'in ', b'her ', b'eyes ', b'\\r', b\"She's \", b'go', b'ing ', b'to ', b'get_you ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is ', b'a', b'no', b'ther ', b'ba', b'by ', b'\\r', b\"She's \", b'gone ', b'to', b'mo', b'rrow ', b'boy ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is ', b'a', b'no', b'ther ', b'ba', b'by ', b'a', b'aa ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is ', b'a', b'no', b'ther ', b'ba', b'by ', b'\\r', b\"She's \", b'gone ', b'to', b'mo', b'rrow ', b'boy ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is{is} ', b'a', b'no{a}', b'ther ', b'{no}ba', b'by ', b'{ther} ', b'a', b'aa{ba', b'by ', b'yea', b'ah} ', b'\\r', b'Ah ', b'ah', b'ah', b'ah ', b'Ah ', b'ah', b'ah', b'ah ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is{is} ', b'a', b'no{a}', b'ther ', b'{no}ba', b'by ', b'{ther} ', b'\\r', b\"She's \", b'gone{ba} ', b'to', b'mo', b'{by}', b'rrow ', b'boy ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is{is} ', b'a', b'no{a}', b'ther ', b'{no}ba', b'by ', b'{ther} ', b'a', b'aa{ba', b'by} ', b'\\r', b'All{a} ', b'that ', b'she ', b'wants ', b'is{is} ', b'a', b'no{a}', b'ther ', b'{no}ba', b'by ', b'{ther} ', b'\\r', b\"She's \", b'gone{ba} ', b'to', b'mo', b'{by}', b'rrow ', b'boy ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'is{is} ', b'a', b'no{a}', b'ther ', b'{no}ba', b'by ', b'{ther} ', b'a', b'aa{ba', b'by ', b'yea', b'ah} ', b'\\r', b'All ', b'that ', b'she ', b'wants ', b'all ', b'that ', b'she ', b'wants ']\n",
      "Notes: ['F#2', 'F#2', 'F#2', 'F#2', 'C#5', '0.3.5', 'C5', 'F5', '0.3', '0.3.5', 'E5', '0.3.5', 'C#5', 'E5', '0.3', 'C#5', 'F5', 'F5', '0.3.5', 'F5', '0.3', '0.3.5', 'F#5', '0.3.5', 'C#5', 'F#5', '0.3', 'C#5', 'F5', 'E5', 'F5', 'C#5', '0.3.5', 'C5', 'F5', '0.3', '0.3.5', 'E5', '0.3.5', 'C#5', 'E5', '0.3', 'C#5', 'F5', 'F5', 'B4', '0.3.5', 'C5', 'F5', '0.3', '0.3.5', '0.2.3.5.6', 'G#2', 'G#2', 'B2', 'C#3', 'C#3', 'C#2', '0.1.3.5.6', '6', '1.4.8', 'C#4', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'G#4', '2.6', '6', '1.4.8', 'F#2', 'F#4', '0.3.5.6', 'D4', '6', 'C#2', 'G#4', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', 'F#4', '0.3.6', '4.6', '2.5.6', '6', '1.4.8', 'G#4', '0.2.3.5.6', 'A4', '6', '0.3.5.6', '6', '8.11.3', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', '0.3.5.6', 'D4', '6', '0.3.5.6', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '6.9.1', 'B1', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', 'C#4', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'G#4', '2.6', '6', '1.4.8', 'F#2', 'F#4', '0.3.5.6', 'D4', '6', 'C#2', 'G#4', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', 'F#4', '0.3.6', '4.6', '2.5.6', '6', '1.4.8', 'G#2', 'G#4', '0.2.3.5.6', 'A4', 'G#2', '6', 'F#2', '0.3.5.6', 'G#2', '6', '8.11.3', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'E2', 'C#7', '0.3.5.6', 'D4', 'E2', 'C#7', '6', 'F#2', 'C#7', '0.3.5.6', 'F#2', 'C#7', '6', '6.9.1', 'C#7', '6', 'D4', 'C#7', '0.3.6', '4.6', 'C#7', '2.5.6', 'C#7', '6', '6.9.1', 'B1', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', '2.6', '6', '1.4.8', 'F#2', '0.3.5.6', 'D4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'G#6', '2.5.6', '6', '1.4.8', 'C#2', 'A6', '0.2.3.5.6', 'A4', 'C#2', '6', 'B1', 'B6', '0.3.5.6', 'B1', '6', '8.11.3', 'E-6', '5.6', 'D4', 'E-6', '0.3.6', 'F#3', '2.6', 'E-6', '6', '8.11.3', 'B1', 'E6', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#6', '0.3.5.6', 'F#1', '6', '6.9.1', 'C#7', '6', 'D4', 'C#7', '0.3.6', '4.6', 'C#7', '2.5.6', 'C#7', '6', '6.9.1', 'B1', 'C#7', '0.2.3.5.6', 'A4', 'C#7', '6', 'C#2', 'C#7', '0.3.5.6', 'C#2', 'C#7', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', '2.6', '6', '1.4.8', 'F#2', '0.3.5.6', 'D4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'G#6', '2.5.6', '6', '1.4.8', 'G#2', 'A6', '0.2.3.5.6', 'A4', 'G#2', '6', 'F#2', 'B6', '0.3.5.6', 'C5', 'G#2', '6', '8.11.3', 'E-6', '5.6', 'D4', 'E-6', '0.3.6', 'F#3', '2.6', 'E-6', '6', '8.11.3', 'E2', 'G#6', '0.3.5.6', 'D4', 'E2', '6', 'F#2', 'F#6', '0.3.5.6', 'F#2', '2.6', 'G#4', 'A4', 'G#4', 'G#4', '0.1.3.5.6', '6', '1.5.8', 'C#4', '5.6', 'D4', '0.3.6', 'F#3', 'C#4', '2.6', '6', '1.5.8', 'G#4', '0.3.5.6', 'D4', 'G#4', '6', 'G#4', '0.3.5.6', '6', '1.5.8', 'C#4', '6', 'D4', 'C#4', '0.3.6', '4.6', '2.5.6', '6', '1.5.8', 'G#4', '0.2.3.5.6', 'A4', 'G#4', '6', 'F#4', '0.3.5.6', '6', '11.3.6', 'F#4', '5.6', 'D4', '0.3.6', 'F#3', 'F#4', '2.6', '6', '11.3.6', 'F4', '0.3.5.6', 'D4', 'F#4', '6', '0.3.5.6', 'F4', '6', '6.10.1', 'E-4', '6', 'D4', 'C#4', '0.3.6', '4.6', '2.5.6', '6', '6.10.1', 'G#3', '0.2.3.5.6', 'A4', '6', 'F4', '0.3.5.6', '6', '1.5.8', 'C#4', '5.6', 'D4', '0.3.6', 'F#3', 'C#4', '2.6', '6', '1.5.8', '0.3.5.6', 'D4', 'F4', '6', '0.3.5.6', '6', '1.5.8', 'C#4', '6', 'D4', '0.3.6', '4.6', 'C#4', '2.5.6', '6', '1.5.8', '0.2.3.5.6', 'A4', '6', '0.3.5.6', '6', '8.0.3', 'E-4', '5.6', 'D4', '0.3.6', 'F#3', 'E-4', '2.6', '6', '8.0.3', 'C#4', '0.3.5.6', 'D4', 'B-3', '6', '0.3.5.6', '6', '8.0', 'G#3', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '8.0', '0.2.3.5.6', 'A4', '6', '0.3.5.6', 'G#4', '6', '1.5.8', 'G#4', '5.6', 'D4', 'G#4', '0.3.6', 'F#3', 'G#4', '2.6', '6', '1.5.8', 'G#4', '0.3.5.6', 'D4', 'C#5', '6', '0.3.5.6', 'G#4', '6', '11.1.5', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '10.1.5', 'G#4', '0.2.3.5.6', 'A4', 'G#4', '6', 'B-4', '0.3.5.6', '6', '6.10.1', 'B-4', '5.6', 'D4', '0.3.6', 'F#3', 'B-4', '2.6', '6', '6.10.1', 'B-4', '0.3.5.6', 'D4', 'A4', '6', '0.3.5.6', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '6.9.11.1', 'F#4', '0.2.3.5.6', 'A4', '6', 'G#4', '0.3.5.6', '6', '1.5.8', 'G#4', '5.6', 'D4', '0.3.6', 'F#3', 'F#4', '2.6', '6', '1.5.8', 'F4', '0.3.5.6', 'D4', 'G#4', '6', '0.3.5.6', '6', '8.0.3', 'G#4', '6', 'D4', '0.3.6', '4.6', 'F#4', '2.5.6', 'F4', '6', '8.0.3', '0.2.3.5.6', 'A4', 'C#4', '6', '0.3.5.6', '6', '6.10.1', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '6.10.1', 'G#3', '0.3.5.6', 'D4', '6', 'G#3', '0.3.5.6', '6', '6.10.1', 'G#3', '6', 'D4', 'G#2', 'E4', '0.3.6', '4.6', 'G#2', '2.5.6', 'B2', 'C#4', '6', 'C#3', '6.10.1', '0.2.3.5.6', 'A4', '6', 'C#3', 'C#2', 'G#4', '0.1.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'E4', '2.5.6', 'E4', '6', '1.4.8', 'C#2', 'E4', '0.2.3.5.6', 'A4', 'C#2', 'E4', '6', 'B1', 'F#4', '0.3.5.6', 'B1', '6', '3.6.8.11', 'F#4', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '3.6.8.11', 'B1', 'E4', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#4', '0.3.5.6', 'F#1', '6', '6.9.1', 'F#4', '6', 'D4', 'F#4', '0.3.6', '4.6', '2.5.6', 'F#4', '6', '6.9.1', 'B1', 'E4', '0.2.3.5.6', 'A4', '6', 'C#2', 'G#4', '0.3.5.6', 'C#2', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'E4', '2.5.6', 'E4', '6', '1.4.8', 'G#2', 'E4', '0.2.3.5.6', 'A4', 'G#2', 'E4', '6', 'F#2', 'E4', '0.3.5.6', 'G#2', '6', '8.11.3', 'E-4', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'E2', 'E-4', '0.3.5.6', 'D4', 'E2', '6', 'F#2', 'C#4', '0.3.5.6', 'F#2', '6', '6.9.1', '6', 'D4', 'G#2', '0.3.6', '4.6', 'G#2', '2.5.6', 'B2', '6', 'C#3', '6.9.1', 'B1', '0.2.3.5.6', 'A4', '6', 'C#3', 'C#2', 'G#4', '0.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'E4', '2.5.6', 'E4', '6', '1.4.8', 'C#2', 'E4', '0.2.3.5.6', 'A4', 'C#2', 'E4', '6', 'B1', 'F#4', '0.3.5.6', 'B1', '6', '3.6.8.11', 'F#4', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '3.6.8.11', 'B1', 'E4', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#4', '0.3.5.6', 'F#1', '6', '6.9.1', 'F#4', '6', 'D4', 'F#4', '0.3.6', '4.6', '2.5.6', 'F#4', '6', '6.9.1', 'B1', 'E4', '0.2.3.5.6', 'A4', '6', 'C#2', 'G#4', '0.3.5.6', 'C#2', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'E4', '2.5.6', 'E4', '6', '1.4.8', 'G#2', 'E4', '0.2.3.5.6', 'A4', 'G#2', 'E4', '6', 'F#2', 'E4', '0.3.5.6', 'G#2', '6', '8.11.3', 'E-4', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'E2', 'E-4', '0.3.5.6', 'D4', 'E2', '6', 'F#2', 'C#4', '0.3.5.6', 'F#2', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '6.9.1', 'B1', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', '0.1.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', '2.6', '6', '1.4.8', 'F#2', '0.3.5.6', 'D4', '6', 'C#2', 'E5', '0.3.5.6', 'C#2', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'C#5', '2.5.6', '6', '1.4.8', 'C#2', '0.2.3.5.6', 'A4', 'C#2', '6', 'B1', '0.3.5.6', 'B1', '6', '3.6.8.11', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '3.6.8.11', 'B1', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#5', '0.3.5.6', 'F#1', 'C#5', '6', '6.9.1', 'F#5', '6', 'D4', '0.3.6', '4.6', 'C#5', '2.5.6', '6', '6.9.1', 'B1', 'E5', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', '0.3.5.6', 'C#2', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', '2.6', '6', '1.4.8', 'F#2', '0.3.5.6', 'D4', '6', 'C#2', 'E5', '0.3.5.6', 'C#2', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'C#5', '2.5.6', '6', '1.4.8', 'G#2', '0.2.3.5.6', 'A4', 'G#2', '6', 'F#2', '0.3.5.6', 'G#2', '6', '8.11.3', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'E2', '0.3.5.6', 'D4', 'E2', '6', 'F#2', '0.3.5.6', 'F#2', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '6.9.1', 'B1', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', '0.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', '2.6', '6', '1.4.8', 'F#2', '0.3.5.6', 'D4', '6', 'C#2', 'E5', '0.3.5.6', 'C#2', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'C#5', '2.5.6', '6', '1.4.8', 'C#2', '0.2.3.5.6', 'A4', 'C#2', '6', 'B1', '0.3.5.6', 'B1', '6', '3.6.8.11', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '3.6.8.11', 'B1', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#5', '0.3.5.6', 'F#1', 'C#5', '6', '6.9.1', 'F#5', '6', 'D4', '0.3.6', '4.6', 'C#5', '2.5.6', '6', '6.9.1', 'B1', 'E5', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', '0.3.5.6', 'C#2', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', '2.6', '6', '1.4.8', 'F#2', '0.3.5.6', 'D4', '6', 'C#2', 'E5', '0.3.5.6', 'C#2', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'C#5', '2.5.6', '6', '1.4.8', 'G#2', '0.2.3.5.6', 'A4', 'G#2', '6', 'F#2', '0.3.5.6', 'G#2', '6', '8.11.3', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'E2', '0.3.5.6', 'D4', 'E2', '6', 'F#2', '0.3.5.6', 'F#2', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '6.9.1', '0.2.3.5.6', 'A4', '6', '0.3.5.6', '5.6', '0.3', '2.6', '0.3.5.6', 'C#4', '0.3.5.6', 'F#3', 'E4', '0.3', '2.5.6', 'F#4', '0.3.5.7', 'G#4', '1.6', '6', '1.4.8', 'C#4', 'F#3', 'F#3', 'C#4', '0.3.6', '6', '1.4.8', 'G#4', 'F#3', 'G#4', 'F#3', 'G#4', '6', '6', '1.4.8', 'C#4', 'F#3', 'C#4', 'F#3', '0.3.6', '6', '1.4.8', 'G#4', 'F#3', 'G#4', 'F#3', 'F#4', '6', '6', '8.11.3', 'F#4', 'F#3', 'F#3', '6.9', '0.3.6', '6', '8.11.3', '11.4', 'F#3', '1.6', 'F#3', '6', '11.4', '6', '6.9.1', '3.9', 'F#3', '1.6', 'F#3', '0.3.6', '6', '6.9.1', 'G#3', 'F#3', 'F#3', 'E4', '6', '6', '1.4.8', 'C#4', 'F#3', 'F#3', 'C#4', '0.3.6', '6', '1.4.8', 'F#3', 'E4', 'F#3', '6', '6', '1.4.8', 'C#4', 'F#3', 'F#3', 'C#4', '0.3.6', '6', '1.4.8', 'F#3', 'F#3', '6', '6', '8.11.3', 'E-4', 'F#3', 'F#3', 'E-4', '0.3.6', '6', '8.11.3', 'C#4', 'F#3', 'B-3', 'F#3', '6', '6', '8.0.3', 'G#3', 'F#3', 'F#3', '0.3.5', 'D2', '8.0.3', '6', 'C#4', '0.1.3.5.6', 'G#4', '6', '1.4.8', 'G#4', '5.6', 'D4', 'G#4', '0.3.6', 'F#3', 'G#4', '2.6', '6', '1.4.8', 'G#4', '0.3.5.6', 'D4', 'C#5', '6', '0.3.5.6', 'G#4', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '1.4.8', 'G#4', '0.2.3.5.6', 'A4', 'G#4', '6', 'A4', 'F#3', '0.3.5.6', '6', '6.9.1', 'A4', '5.6', 'D4', '0.3.6', 'F#3', 'A4', '2.6', '6', '6.9.1', 'G#4', '0.3.5.6', 'D4', 'F#4', '6', 'A3', '0.3.5.6', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', '2.5.6', '6', '6.9.1', 'F#4', '0.2.3.5.6', 'A4', '6', 'G#4', 'G#3', '0.3.5.6', '6', '1.4.8', 'G#4', '5.6', 'D4', '0.3.6', 'F#3', 'F#4', '2.6', '6', '1.4.8', 'E4', '0.3.5.6', 'D4', 'G#4', '6', '0.3.5.6', '6', '8.11.3', 'G#4', '6', 'D4', '0.3.6', '4.6', 'F#4', '2.5.6', 'E4', '6', '8.11.3', '0.2.3.5.6', 'A4', 'C#4', 'F#3', '6', '0.3.5.6', 'C5', '6', '6.9.1', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '6.9.1', 'G#3', '0.3.5.6', 'D4', '6', 'G#3', '0.2.3.5.6', 'G#3', 'G#2', 'E4', 'G#2', 'B2', 'C#4', 'C#3', 'C#3', 'C#2', 'G#4', '0.1.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'E4', '2.5.6', 'E4', '6', '1.4.8', 'C#2', 'E4', '0.2.3.5.6', 'A4', 'C#2', 'E4', '6', 'B1', 'F#4', '0.3.5.6', 'B1', '6', '3.6.8.11', 'F#4', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '3.6.8.11', 'B1', 'E4', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#4', '0.3.5.6', 'F#1', '6', '6.9.1', 'F#4', '6', 'D4', 'F#4', '0.3.6', '4.6', '2.5.6', 'F#4', '6', '6.9.1', 'B1', 'E4', '0.2.3.5.6', 'A4', '6', 'C#2', 'G#4', '0.3.5.6', 'C#2', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'E4', '2.5.6', 'E4', '6', '1.4.8', 'G#2', 'E4', '0.2.3.5.6', 'A4', 'G#2', 'E4', '6', 'F#2', 'E4', '0.3.5.6', 'G#2', '6', '8.11.3', 'E-4', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'E2', 'E-4', '0.3.5.6', 'D4', 'E2', '6', 'F#2', 'C#4', '0.3.5.6', 'F#2', '6', '6.9.1', '6', 'D4', 'G#2', '0.3.6', '4.6', 'G#2', '2.5.6', 'B2', '6', 'C#3', '6.9.1', 'B1', '0.2.3.5.6', 'A4', '6', 'C#3', 'C#2', 'G#4', '0.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', 'E4', '2.5.6', 'E4', '6', '1.4.8', 'C#2', 'E4', '0.2.3.5.6', 'A4', 'C#2', 'E4', '6', 'B1', 'F#4', '0.3.5.6', 'B1', '6', '3.6.8.11', 'F#4', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '3.6.8.11', 'B1', 'E4', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#4', '0.3.5.6', 'F#1', '6', '6.9.1', 'F#4', '6', 'D4', 'F#4', '0.3.6', '4.6', '2.5.6', 'F#4', '6', '6.9.1', 'B1', 'E4', '0.2.3.5.6', 'A4', '6', 'C#2', 'G#4', '0.3.5.6', 'C#2', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', '0.3.5.6', 'C#2', '6', '1.4.8', '6', 'D4', '0.3.6', '4.6', '4.8', '2.5.6', 'E4', '6', '1.4.8', 'G#2', '11.4', '0.2.3.5.6', 'A4', 'G#2', 'E4', '6', 'F#2', 'E4', 'C#5', '0.3.5.6', 'C5', 'G#2', '6', '8.11.3', 'E-4', '5.6', 'D4', '0.3.6', 'F#3', 'C#5', '2.6', '6', '8.11.3', 'E2', 'E-4', '0.3.5.6', 'D4', 'E2', '6', 'F#2', 'C#4', 'F#4', '0.2.3.5.6', 'F#2', '6.9.1', 'F#4', 'B1', 'C#2', 'E4', '0.1.3.5.6', 'C#4', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', 'G#4', '6', 'D4', '0.3.6', '4.6', 'B1', 'F#4', '2.5.6', 'E4', '6', 'C2', 'C#4', '0.2.3.5.6', 'A4', '6', 'C#2', '8.1', '0.3.5.6', '6', 'C#2', '8.1', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '8.1', '0.3.5.6', 'D4', '6', 'G#1', '3.8', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '6.11', '2.5.6', '6', 'C2', '7.0', '0.2.3.5.6', 'A4', '6', 'C#2', '8.1', '0.3.5.6', '6', 'C#2', '8.1', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '8.1', '0.3.5.6', 'D4', '6', 'G#1', '3.8', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '6.11', '2.5.6', '6', 'C2', '7.0', '0.2.3.5.6', 'A4', '6', 'C#2', '8.1', '0.3.5.6', '6', 'C#2', '8.1', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '8.1', '0.3.5.6', 'D4', '6', 'G#1', '3.8', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '6.11', '2.5.6', '6', 'C2', '7.0', '0.2.3.5.6', 'A4', '6', 'C#2', '8.1', '0.3.5.6', '6', 'C#2', '8.1', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', 'G#4', '6', 'D4', '0.3.6', '4.6', 'B1', 'F#4', '2.5.6', 'E4', '6', 'C2', 'C#4', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', 'G#4', '0.1.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', 'E5', '0.3.5.6', 'C#2', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'C#5', '4.8', '2.5.6', 'E4', '6', '1.4.8', 'C#2', '11.4', '0.2.3.5.6', 'A4', 'C#2', 'E4', '6', 'B1', 'F#4', 'C#5', '0.3.5.6', 'B1', '6', '3.6.8.11', 'F#4', '5.6', 'D4', '0.3.6', 'F#3', 'C#5', '2.6', '6', '3.6.8.11', 'B1', 'E4', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#5', 'F#4', '0.3.5.6', 'F#1', 'C#5', '6', '6.9.1', 'F#5', 'F#4', '6', 'D4', 'F#4', '0.3.6', '4.6', 'C#5', 'F#4', '2.5.6', 'F#4', '6', '6.9.1', 'B1', 'E5', 'E4', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', 'G#4', '0.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', 'E5', '0.3.5.6', 'C#2', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'C#5', '4.8', '2.5.6', 'E4', '6', '1.4.8', 'G#2', '11.4', '0.2.3.5.6', 'A4', 'G#2', 'E4', '6', 'F#2', 'E4', 'C#5', '0.3.5.6', 'G#2', '6', '8.11.3', 'E-4', '5.6', 'D4', '0.3.6', 'F#3', 'C#5', '2.6', '6', '8.11.3', 'E2', 'E-4', '0.3.5.6', 'D4', 'E2', '6', 'F#2', 'C#4', 'F#4', '0.3.5.6', 'F#2', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', 'F#4', '2.5.6', '6', '6.9.1', 'B1', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', 'E4', 'G#4', '0.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', 'E5', '0.3.5.6', 'C#2', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'C#5', '4.8', '2.5.6', 'E4', '6', '1.4.8', 'C#2', '11.4', '0.2.3.5.6', 'A4', 'C#2', 'E4', '6', 'B1', 'F#4', 'C#5', '0.3.5.6', 'B1', '6', '3.6.8.11', 'F#4', '5.6', 'D4', '0.3.6', 'F#3', 'C#5', '2.6', '6', '3.6.8.11', 'B1', 'E4', '0.3.5.6', 'D4', 'B1', '6', 'F#1', 'F#5', 'F#4', '0.3.5.6', 'F#1', 'C#5', '6', '6.9.1', 'F#5', 'F#4', '6', 'D4', 'F#4', '0.3.6', '4.6', 'C#5', 'F#4', '2.5.6', 'F#4', '6', '6.9.1', 'B1', 'E5', 'E4', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', 'G#4', '0.3.5.6', '6', '1.4.8', '5.6', 'D4', 'C#2', '0.3.6', 'F#3', 'E2', 'F#4', '2.6', '6', '1.4.8', 'F#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'C#2', 'E5', '0.3.5.6', 'C#2', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'C#5', '4.8', '2.5.6', 'E4', '6', '1.4.8', 'G#2', '11.4', '0.2.3.5.6', 'A4', 'G#2', 'E4', '6', 'F#2', 'E4', 'C#5', '0.3.5.6', 'G#2', '6', '8.11.3', 'E-4', '5.6', 'D4', '0.3.6', 'F#3', 'C#5', '2.6', '6', '8.11.3', 'E2', 'E-4', '0.3.5.6', 'D4', 'E2', '6', 'F#2', 'C#4', 'F#4', '0.3.5.6', 'F#2', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', 'F#4', '2.5.6', '6', '6.9.1', 'B1', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', 'E4', '0.3.5.6', 'C#4', '6', '1.4.8', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '1.4.8', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', 'E5', '0.3.5.6', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'B1', 'C#5', '2.5.6', '6', '1.4.8', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', '8.11.3', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', 'F#5', '0.3.5.6', 'C#5', '6', '6.9.1', 'F#5', '6', 'D4', '0.3.6', '4.6', 'B1', 'C#5', '2.5.6', '6', '6.9.1', 'C2', 'E5', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', '0.3.5.6', '6', '1.4.8', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '1.4.8', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', 'E5', '0.3.5.6', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'B1', 'C#5', '2.5.6', '6', '1.4.8', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', '8.11.3', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', '6.9.1', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', 'G#4', '0.3.5.6', '6', '1.4.8', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', 'F#4', '2.6', '6', '1.4.8', 'C#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'G#1', 'E5', '0.3.5.6', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'B1', 'C#5', '2.5.6', '6', '1.4.8', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', '8.11.3', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', 'F#5', '0.3.5.6', 'C#5', '6', '6.9.1', 'F#5', '6', 'D4', '0.3.6', '4.6', 'B1', 'C#5', '2.5.6', '6', '6.9.1', 'C2', 'E5', '0.2.3.5.6', 'A4', '6', 'C#2', 'C#5', 'G#4', '0.3.5.6', '6', '1.4.8', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', 'F#4', '2.6', '6', '1.4.8', 'C#2', 'E4', '0.3.5.6', 'D4', 'C#4', '6', 'G#1', 'E5', '0.3.5.6', 'C#5', '6', '1.4.8', 'E5', '6', 'D4', '0.3.6', '4.6', 'B1', 'C#5', '2.5.6', '6', '1.4.8', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', '8.11.3', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', '8.11.3', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6.9.1', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', '6.9.1', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6', 'C#2', '0.3.5.6', '6', 'C#2', '5.6', 'D4', '0.3.6', 'F#3', '2.6', '6', 'C#2', '0.3.5.6', 'D4', '6', 'G#1', '0.3.5.6', '6', '6', 'D4', '0.3.6', '4.6', 'B1', '2.5.6', '6', 'C2', '0.2.3.5.6', 'A4', '6']\n",
      "For file test_midi_small/14_Years.mid extracted:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [b'I ', b'try ', b'and ', b'feel ', b'the ', b'sun', b'shine ', b'\\r', b'You ', b'bring ', b'the ', b'rain ', b'\\r', b'You ', b'try ', b'and ', b'hold ', b'me ', b'down ', b'\\r', b'With ', b'your ', b'com', b'plaints ', b'\\r', b'You ', b'cry ', b'and ', b'moan ', b'and ', b'com', b'plain ', b'\\r', b'You ', b'whine ', b'an ', b'tear ', b'\\r', b'Up ', b'to ', b'my ', b'neck ', b'in ', b'sor', b'row ', b'\\r', b'The ', b'touch ', b'you ', b'bring ', b'\\r', b'You ', b'just ', b\"don't \", b'step ', b'in', b'side ', b'to ', b'\\r', b'(to) ', b'Four', b'teen ', b'ye', b'ars ', b'\\r', b'So ', b'hard ', b'to ', b'keep ', b'my ', b'o', b'wn ', b'head... ', b'\\r', b\"That's \", b'what ', b'I ', b'say ', b'\\r', b'You ', b'kno', b'w... ', b\"I've \", b'been ', b'the ', b'beg', b'gar... ', b'\\r', b\"I've \", b'played ', b'the ', b'thief ', b'\\r', b'I ', b'was ', b'the ', b'dog... ', b'they ', b'all ', b'tried ', b'to ', b'beat ', b'\\r', b'But ', b\"it's \", b'bee', b'n ', b'\\r', b'Four', b'teen ', b'years ', b'of ', b'si', b'lence ', b'\\r', b\"It's \", b'been ', b'\\r', b'Four', b'teen ', b'years ', b'of ', b'pain ', b'\\r', b\"It's \", b'been ', b'\\r', b'Four', b'teen ', b'years ', b'that ', b'are ', b'gone ', b'for', b'e', b'ver ', b'(e', b'ver) ', b'\\r', b\"And_I'll \", b'never ', b'have ', b'a', b'gain ', b'(well) ', b'\\r', b' ', b'\\r', b'Your ', b'stu', b'pid ', b'girl', b'friends ', b'tell ', b'you ', b'\\r', b'That ', b\"I'm \", b'to ', b'blame ', b'\\r', b'Well ', b\"they're \", b'all ', b'used', b'up ', b'has~beens ', b'\\r', b'Out ', b'of ', b'the ', b'game ', b'\\r', b'This ', b'time ', b\"I'll \", b'have ', b'the ', b'last ', b'word ', b'\\r', b'You_hear ', b'what ', b'I ', b'say ', b'\\r', b'I ', b'tried ', b'to ', b'see ', b'it ', b'your ', b'way ', b'\\r', b\"It_won't \", b'work ', b'to', b'day ', b'\\r', b'You ', b'just ', b\"don't \", b'step ', b'in', b'side ', b'to ', b'\\r', b'(to) ', b'Four', b'teen ', b'ye', b'ars ', b'\\r', b'So ', b'hard ', b'to ', b'keep ', b'my ', b'o', b'wn ', b'head... ', b'\\r', b\"That's \", b'what ', b'I ', b'say ', b'\\r', b'You ', b'know... ', b\"I've \", b'been ', b'the ', b'dea', b'ler... ', b'\\r', b'Ha', b\"ngin' \", b'on ', b'your ', b'street ', b'\\r', b'I ', b'was ', b'the ', b'dog... ', b'they ', b'all ', b'tried ', b'to ', b'beat ', b'\\r', b'But ', b\"it's \", b'bee', b'n ', b'\\r', b'Four', b'teen ', b'years ', b'of ', b'si', b'lence ', b'\\r', b\"It's \", b'been ', b'\\r', b'Four', b'teen ', b'years ', b'of ', b'pain ', b'\\r', b\"It's \", b'been ', b'\\r', b'Four', b'teen ', b'years ', b'that ', b'are ', b'gone ', b'for', b'e', b'ver ', b'(e', b'ver) ', b'\\r', b\"And_I'll \", b'never ', b'have ', b'a', b'gain ', b'(well) ', b'\\r', b' ', b'\\r', b'Bull', b'shit ', b'and ', b'con', b'tem', b'pla', b'tion ', b'\\r', b'Gos', b\"sip's \", b'their ', b'trade ', b'\\r', b'If ', b'they ', b'knew ', b'half ', b'the ', b'real_truth ', b'\\r', b'What ', b'would ', b'they ', b'say ', b'\\r', b'Well ', b\"I'm_past \", b'the ', b'point ', b'of ', b'con', b'cern ', b'\\r', b\"It's \", b'time ', b'to ', b'play ', b'\\r', b'These ', b'last ', b'four ', b'years ', b'of ', b'mad', b'ness ', b'\\r', b'Sure ', b'put ', b'me ', b'straight ', b'\\r', b\"Don't \", b'get ', b'back ', b'four', b'teen ', b'ye', b'ars ', b'\\r', b'In ', b'just ', b'one ', b'da', b'y ', b'\\r', b'So ', b'hard ', b'to ', b'keep ', b'my ', b'o', b'wn ', b'head ', b'\\r', b'Just ', b'go ', b'a', b'way ', b'\\r', b'You ', b'kno', b'w... ', b'just ', b'like ', b'a ', b'hoo', b'ker ', b'she_said ', b'\\r', b'No', b\"thin's \", b'for ', b'free ', b'\\r', b'Oh ', b'I ', b'tried ', b'to ', b'see_it_your_way ', b'\\r', b'I ', b'tried_to ', b'see ', b'it ', b'your_way ']\n",
      "Notes: ['D2', 'D2', '7.11', '5.11', 'D2', 'D2', '11', '7.11', 'G2', 'D2', 'D2', 'D2', 'C2', 'A2', 'A2', 'A3', '9.0.4', 'A2', '9.2', 'A2', 'A3', '9.0', 'C2', 'G2', '7.11.2', 'G2', 'G3', 'C5', '4.7.11', 'G2', 'G2', '4.7.11', 'G3', 'C2', 'F2', 'F2', 'F3', '5.9.0', 'F#2', 'F2', '2.5.9', 'F2', 'F3', 'C5', '4.8.11', 'C2', 'E2', 'E2', 'E3', 'F#2', 'E2', 'E2', 'E3', 'C2', 'A2', 'A2', 'A3', '9.0.4', 'F#2', 'A2', '9.2', 'A2', 'A3', '9.0', 'C2', 'G2', '7.11.2', 'G2', 'G3', 'C5', '4.7.11', 'F#2', 'G2', 'G2', 'G3', 'C2', 'F2', 'F2', 'F3', '5.9.0', 'F#2', 'F2', '2.5.9', 'F2', 'F3', 'C5', '4.8.11', 'C2', 'E2', 'E2', 'E3', 'D2', 'E2', 'D2', 'E2', 'D2', 'D2', 'E3', 'C2', 'A2', 'A2', 'C2', 'B-2', 'A3', '9.0.4', 'A2', '9.2', 'A2', 'C2', 'C2', 'A3', '9.0', 'C2', 'G2', '7.11.2', 'G2', 'C2', 'B-2', 'G3', 'C5', '4.7.11', 'F#2', 'G2', 'G2', '4.7.11', 'C2', 'C2', 'B-2', 'G3', 'C2', 'F#2', 'F2', 'F2', 'C2', 'B-2', 'F3', '5.9.0', 'F#2', 'F2', '2.5.9', 'F2', 'C2', 'C2', 'B-2', 'F3', 'C5', '4.8.11', 'C2', 'F#2', 'E2', 'E2', 'B-2', 'C2', 'E3', 'C2', 'F#2', 'E2', 'E2', 'C2', 'C2', 'B-2', 'E3', 'F#2', 'C2', 'A2', 'A2', 'B-2', 'C2', 'A3', '9.0.4', 'F#2', 'A2', '9.2', 'A2', '9.0', 'C2', 'C2', 'B-2', 'A3', '7.11.2', 'F#2', 'C2', 'G2', 'G2', '7.0', 'B-2', 'C2', 'G3', '4.7.11', 'F#2', 'G2', 'G2', '4.7.11', 'C2', 'B-2', 'C2', 'G3', 'F#2', 'C2', 'F2', 'F2', 'B-2', 'C2', 'F3', '5.9.0', '2.5.9', 'F#2', 'F2', 'F2', '5.9.0', 'C2', 'B-2', 'C2', 'F3', '4.8.11', 'F#2', 'C2', 'E2', 'E2', 'B-2', 'C2', 'E3', 'F#2', 'C2', 'E2', 'E2', 'D2', 'D2', 'E3', 'D2', 'D2', 'C2', 'A2', '4.9', 'C2', 'A2', 'E5', 'E5', 'C2', 'A2', 'G5', 'C2', 'C2', 'A2', 'C2', '7.0', 'C3', 'A5', 'G5', 'C2', 'C3', 'E5', 'C2', 'C3', 'C2', 'C2', 'C3', 'C5', 'C2', '2.6.9', 'D3', 'C2', 'D3', 'D5', 'D5', 'C2', 'D3', 'C5', 'C2', 'C2', 'D3', 'E5', 'C2', 'D3', 'A4', 'C2', 'D3', 'C2', 'D3', 'C2', 'C2', 'D3', 'C2', 'A2', '4.9', 'C2', 'A2', 'E5', 'C2', 'A2', 'E5', 'C2', 'C2', 'A2', 'G5', 'A5', 'C2', '7.0', 'C3', 'G5', 'C2', 'C3', 'E5', 'C2', 'C3', 'D5', 'C2', 'C2', 'C3', 'C5', 'C2', '2.6.9', 'D3', 'C2', 'D3', 'D5', 'C2', 'D3', 'D5', 'C5', 'C2', 'C2', 'D3', 'E5', 'C2', 'D3', 'C2', 'D3', 'A4', 'C5', 'C2', 'D3', 'D5', 'C2', 'C2', 'D3', 'C2', 'A2', '4.9', 'C2', 'A2', 'E5', 'C2', 'A2', 'D5', 'C2', 'C2', 'A2', 'C5', 'E5', 'C2', '7.0', 'C3', 'E5', 'C2', 'C3', 'C2', 'C3', 'C2', 'C2', 'C3', 'C2', 'C2', '2.6.9', 'D3', 'C2', 'D3', 'D5', 'C2', 'D3', 'D5', 'C2', 'C2', 'D3', 'C5', 'E5', 'C2', 'D3', 'C2', 'D3', 'C2', 'D3', 'C2', 'C2', 'D3', 'C2', 'B-2', 'A2', '4.9', 'C2', 'F#2', 'A2', 'E5', 'B-2', 'C2', 'A2', 'D5', 'C2', 'C2', 'F#2', 'A2', 'C5', 'E5', 'C2', 'C2', 'B-2', '7.0', 'C3', 'C2', 'F#2', 'C3', 'C2', 'B-2', 'C3', 'C2', 'F#2', 'C2', 'C3', 'B-2', 'C2', '2.6.9', 'D3', 'D5', 'C2', 'F#2', 'D3', 'D5', 'B-2', 'C2', 'D3', 'D5', 'C2', 'F#2', 'C2', 'D3', 'C5', 'E5', 'C2', 'B-2', 'C2', 'D3', 'C2', 'F#2', 'C2', 'D3', 'A5', 'B-2', 'C2', 'D3', 'A5', 'D2', 'D2', 'D3', 'A5', 'D2', 'G5', 'D2', 'C2', 'B-2', '4.9', 'G5', 'F#2', 'G5', 'B-2', 'C2', 'G5', 'F#2', 'E5', 'C2', 'A5', 'C2', 'B-2', 'G3', '2.7', 'F#2', 'G5', 'C2', 'B-2', 'G5', 'A5', 'F#2', 'B-2', 'C2', 'G3', '9.2', 'G5', 'F#2', 'A5', 'B-2', 'C2', 'A5', 'F#2', 'G5', 'C6', 'C2', 'B-2', 'F#2', 'C2', 'C2', 'A5', 'A5', 'C2', 'C2', 'C2', 'B-2', 'G3', '4.9', 'G5', 'F#2', 'A5', 'C2', 'B-2', 'A5', 'F#2', 'A5', 'G5', 'B-2', 'C2', 'G3', '2.7', 'A5', 'F#2', 'G5', 'A5', 'B-2', 'C2', 'G5', 'F#2', 'A5', 'G5', 'B-2', 'C2', 'G3', '9.2', 'F#2', 'A5', 'G5', 'C2', 'B-2', 'A5', 'F#2', 'G5', 'C6', 'D2', 'D2', 'D2', 'D2', 'C2', 'D2', 'D2', 'D2', 'A5', 'D2', 'C2', 'B-2', 'C2', 'A2', 'A2', 'F#2', 'C2', 'A3', '4.9', 'C2', 'B-2', 'C2', 'A2', '4.7.9', 'A2', '4.9', 'C2', 'C2', 'F#2', 'A3', '2.7', 'C2', 'C2', 'B-2', 'G2', 'G2', 'C2', 'C2', 'F#2', 'G3', '2.7', 'C2', 'B-2', 'C2', 'G2', '2.5.7', 'G2', '2.7', 'C2', 'C2', 'F#2', 'G3', '0.5', 'C2', 'B-2', 'C2', 'F2', 'F2', 'C2', 'F#2', 'C2', 'F3', '0.5', 'C2', 'B-2', 'C2', 'F2', '5.7.0', 'F2', '0.5', 'C2', 'F#2', 'C2', 'F3', '11.4', 'C2', 'B-2', 'C2', 'E2', 'E2', 'C2', 'F#2', 'C2', 'E3', '11.4', 'C2', 'C2', 'B-2', 'E2', '11.2.4', 'E2', '11.4', 'C2', 'C2', 'F#2', 'E3', 'C2', 'C2', 'B-2', 'A2', 'A2', 'C2', 'C2', 'F#2', 'A3', '4.9', 'C2', 'B-2', 'C2', 'A2', '4.7.9', 'A2', '4.9', 'C2', 'C2', 'F#2', 'A3', '2.7', 'C2', 'B-2', 'C2', 'G2', 'G2', 'C2', 'C2', 'F#2', 'G3', '2.7', 'C2', 'C2', 'B-2', 'G2', '2.5.7', 'G2', '2.7', 'C2', 'F#2', 'C2', 'G3', '0.5', 'C2', 'B-2', 'C2', 'F2', 'F2', 'C2', 'F#2', 'F3', '0.5', 'C2', 'B-2', 'C2', 'F2', '5.7.0', 'F2', '0.5', 'C2', 'F#2', 'C2', 'F3', '11.4', 'C2', 'B-2', 'C2', 'E2', 'E2', 'C2', 'F#2', 'E3', '11.4', 'C2', 'D2', 'E2', '11.2.4', 'D2', 'E2', '11.4', 'D2', 'D2', 'E3', 'C2', 'B-2', 'C2', 'A2', 'A2', 'C2', 'C2', 'F#2', 'A3', '9.0.4', 'C2', 'C2', 'B-2', 'A2', '9.2', 'A2', 'C2', 'F#2', 'C2', 'A3', '9.0', 'C2', 'C2', 'B-2', 'G2', '7.11.2', 'G2', 'C2', 'C2', 'F#2', 'G3', 'C5', '4.7.11', 'C2', 'C2', 'B-2', 'G2', 'G2', '4.7.11', 'C2', 'C2', 'F#2', 'G3', 'C2', 'B-2', 'C2', 'F2', 'F2', 'C2', 'C2', 'F#2', 'F3', '5.9.0', 'C2', 'B-2', 'C2', 'F2', '2.5.9', 'F2', 'C2', 'F#2', 'C2', 'F3', 'C5', '4.8.11', 'C2', 'C2', 'B-2', 'E2', 'E2', 'F#2', 'C2', 'E3', 'C2', 'B-2', 'E2', 'E2', 'C2', 'C2', 'F#2', 'E3', 'C2', 'C2', 'B-2', 'A2', 'A2', 'C2', 'F#2', 'C2', 'A3', '9.0.4', 'C2', 'B-2', 'C2', 'A2', '9.2', 'A2', 'C2', 'C2', 'F#2', 'A3', '9.0', 'C2', 'C2', 'B-2', 'G2', '7.11.2', 'G2', 'C2', 'F#2', 'C2', 'G3', 'C5', '4.7.11', 'C2', 'C2', 'B-2', 'G2', 'G2', 'C2', 'C2', 'F#2', 'G3', 'C2', 'C2', 'B-2', 'F2', 'F2', 'C2', 'F#2', 'C2', 'F3', '5.9.0', 'C2', 'B-2', 'C2', 'F2', '2.5.9', 'F2', 'C2', 'C2', 'F#2', 'F3', 'C5', '4.8.11', 'C2', 'C2', 'B-2', 'E2', 'E2', 'C2', 'F#2', 'E3', 'C2', 'B-2', 'E2', 'E2', 'C2', 'F#2', 'C2', 'E3', 'C2', 'C2', 'B-2', 'A2', 'A2', 'F#2', 'C2', 'A3', '9.0.4', 'C2', 'B-2', 'A2', '9.2', 'A2', 'C2', 'C2', 'F#2', 'A3', '9.0', 'B-2', 'C2', 'G2', '7.11.2', 'G2', 'F#2', 'C2', 'G3', 'C5', '4.7.11', 'B-2', 'C2', 'G2', 'G2', '4.7.11', 'C2', 'C2', 'F#2', 'G3', 'B-2', 'C2', 'F2', 'F2', 'C2', 'F#2', 'F3', '5.9.0', 'C2', 'B-2', 'F2', '2.5.9', 'F2', 'C2', 'F#2', 'C2', 'F3', 'C5', '4.8.11', 'C2', 'B-2', 'E2', 'E2', 'C2', 'F#2', 'E3', 'C2', 'B-2', 'E2', 'E2', 'C2', 'F#2', 'C2', 'E3', 'C2', 'B-2', 'A2', 'A2', 'F#2', 'C2', 'A3', '9.0.4', 'C2', 'B-2', 'A2', '9.2', 'A2', '9.0', 'C2', 'C2', 'F#2', 'A3', '7.11.2', 'C2', 'B-2', 'G2', 'G2', '7.0', 'C2', 'F#2', 'G3', '4.7.11', 'C2', 'B-2', 'G2', 'G2', '4.7.11', 'C2', 'C2', 'F#2', 'G3', 'C2', 'C2', 'B-2', 'F2', 'F2', 'F#2', 'C2', 'F3', '5.9.0', '2.5.9', 'B-2', 'C2', 'F2', 'F2', '5.9.0', 'C2', 'F#2', 'C2', 'F3', '4.8.11', 'C2', 'C2', 'B-2', 'E2', 'E2', 'C2', 'F#2', 'C2', 'E3', 'C2', 'C2', 'B-2', 'E2', 'E2', 'D2', 'D2', 'E3', 'D2', 'D2', 'B-2', 'C2', 'A2', '4.9', 'F#2', 'C2', 'A2', 'E5', 'E5', 'B-2', 'C2', 'A2', 'G5', 'C2', 'F#2', 'C2', 'A2', 'C2', 'B-2', 'C2', '7.0', 'C3', 'A5', 'G5', 'F#2', 'C2', 'C3', 'E5', 'C2', 'B-2', 'C2', 'C3', 'C2', 'F#2', 'C2', 'C3', 'C5', 'C2', 'B-2', 'C2', '2.6.9', 'D3', 'F#2', 'C2', 'D3', 'D5', 'D5', 'C2', 'C2', 'B-2', 'D3', 'C5', 'C2', 'F#2', 'C2', 'D3', 'E5', 'C2', 'B-2', 'C2', 'D3', 'A4', 'F#2', 'C2', 'D3', 'C2', 'C2', 'B-2', 'D3', 'C2', 'F#2', 'C2', 'D3', 'C2', 'B-2', 'C2', 'A2', '4.9', 'F#2', 'C2', 'A2', 'E5', 'B-2', 'C2', 'A2', 'E5', 'C2', 'F#2', 'C2', 'A2', 'G5', 'A5', 'B-2', 'C2', '7.0', 'C3', 'G5', 'C2', 'F#2', 'C2', 'C3', 'E5', 'C2', 'B-2', 'C2', 'C3', 'D5', 'C2', 'C2', 'F#2', 'C3', 'C5', 'C2', 'B-2', 'C2', '2.6.9', 'D3', 'F#2', 'C2', 'D3', 'D5', 'B-2', 'C2', 'D3', 'D5', 'C5', 'C2', 'F#2', 'C2', 'D3', 'E5', 'C2', 'B-2', 'C2', 'D3', 'C2', 'F#2', 'C2', 'D3', 'A4', 'C5', 'C2', 'B-2', 'C2', 'D3', 'D5', 'D2', 'D2', 'D3', 'D2', 'D2', 'B-2', 'C2', 'A2', '4.9', 'F#2', 'C2', 'A2', 'E5', 'B-2', 'C2', 'A2', 'D5', 'C2', 'F#2', 'C2', 'A2', 'C5', 'E5', 'C2', 'B-2', 'C2', '7.0', 'C3', 'E5', 'F#2', 'C2', 'C3', 'B-2', 'C2', 'C3', 'C2', 'F#2', 'C2', 'C3', 'C2', 'C2', 'B-2', '2.6.9', 'D3', 'F#2', 'C2', 'D3', 'D5', 'C2', 'C2', 'B-2', 'D3', 'D5', 'C2', 'C2', 'F#2', 'D3', 'C5', 'E5', 'C2', 'C2', 'B-2', 'D3', 'C2', 'C2', 'F#2', 'D3', 'C2', 'B-2', 'C2', 'D3', 'C2', 'C2', 'F#2', 'D3', 'C2', 'B-2', 'C2', 'A2', '4.9', 'C2', 'F#2', 'A2', 'E5', 'C2', 'B-2', 'A2', 'D5', 'C2', 'C2', 'F#2', 'A2', 'C5', 'E5', 'C2', 'C2', 'B-2', '7.0', 'C3', 'F#2', 'C2', 'C3', 'B-2', 'C2', 'C3', 'C2', 'F#2', 'C2', 'C3', 'C2', 'C2', 'B-2', '2.6.9', 'D3', 'D5', 'C2', 'F#2', 'C2', 'D3', 'D5', 'C2', 'C2', 'B-2', 'D3', 'D5', 'C2', 'C2', 'F#2', 'D3', 'C5', 'E5', 'C2', 'C2', 'B-2', 'D3', 'C2', 'F#2', 'C2', 'D3', 'A5', 'C2', 'D2', 'C2', 'D3', 'A5', 'D2', '0.2', 'C2', 'D2', 'D3', 'A5', 'D2', 'G5', 'C2', 'B-2', 'G3', '4.9', 'G5', 'F#2', 'G5', 'C2', 'B-2', 'G5', 'F#2', 'E5', 'C2', 'A5', 'C2', 'B-2', '2.7', 'F#2', 'G5', 'B-2', 'C2', 'G5', 'A5', 'F#2', 'B-2', 'C2', 'G3', '9.2', 'G5', 'F#2', 'A5', 'C2', 'B-2', 'A5', 'F#2', 'G5', 'C6', 'B-2', 'C2', 'F#2', '10.0', 'A5', 'A5', 'D2', 'C2', 'D2', 'D2', 'D2', 'C2', 'B-2', 'G3', '4.9', 'G5', 'F#2', 'A5', 'C2', 'B-2', 'A5', 'F#2', 'A5', 'G5', 'C2', 'B-2', 'G3', '2.7', 'A5', 'F#2', 'G5', 'A5', 'B-2', 'C2', 'G5', 'F#2', 'A5', 'G5', 'C2', 'D2', '9.2', 'D2', 'D2', 'A5', 'G5', 'C2', 'D2', 'A5', 'D2', 'D2', 'G5', 'C2', 'C6', 'D2', 'D2', 'D2', 'D2', 'C2', 'D2', 'D2', 'D2', 'D2', 'D2', 'A5', 'D2', 'C2', 'D2', 'D2', 'D2', '7.0', 'A2', 'A2', 'C2', 'A3', '4.9', 'C2', 'A2', '4.7.9', 'A2', '4.9', 'C2', '7.0', 'A3', '2.7', 'C2', 'G2', 'G2', 'C2', 'G3', '2.7', 'C2', 'G2', '2.5.7', 'G2', '2.7', 'C2', '7.0', 'G3', '0.5', 'C2', 'F2', 'F2', 'C2', 'F3', '0.5', 'C2', 'F2', '5.7.0', 'F2', '0.5', 'C2', '7.0', 'F3', '11.4', 'C2', 'E2', 'E2', 'C2', 'E3', '11.4', 'C2', 'E2', '11.2.4', 'E2', '11.4', 'C2', '7.0', 'E3', 'C2', 'A2', 'A2', 'C2', 'A3', '4.9', 'C2', 'A2', '4.7.9', 'A2', '4.9', 'C2', '7.0', 'A3', '2.7', 'C2', 'G2', 'G2', 'C2', 'G3', '2.7', 'C2', 'G2', '2.5.7', 'G2', '2.7', 'C2', '7.0', 'G3', '0.5', 'C2', 'F2', 'F2', 'C2', 'F3', '0.5', 'C2', 'F2', '5.7.0', 'F2', '0.5', 'C2', 'C2', 'F3', '11.4', 'C2', 'C2', 'E2', 'E2', 'D2', 'D2', 'E3', '11.4', 'D2', 'D2', 'D2', 'E2', '11.2.4', 'E2', '11.4', 'D2', 'D2', 'E3', 'D2', 'D2', 'C2', 'D2', 'A2', 'A2', 'A3', '9.0.4', 'B-2', 'A2', '9.2', 'A2', 'F#2', 'A3', '9.0', 'B-2', 'C2', 'G2', '7.11.2', 'G2', 'F#2', 'G3', 'C5', '4.7.11', 'B-2', 'G2', 'G2', '4.7.11', 'F#2', 'G3', 'C2', 'B-2', 'F2', 'F2', 'F#2', 'F3', '5.9.0', 'B-2', 'F2', '2.5.9', 'F2', 'F#2', 'F3', 'C5', '4.8.11', 'B-2', 'C2', 'E2', 'E2', 'F#2', 'E3', 'B-2', 'E2', 'E2', 'F#2', 'E3', 'C2', 'B-2', 'A2', 'A2', 'F#2', 'A3', '9.0.4', 'B-2', 'A2', '9.2', 'A2', 'F#2', 'A3', '9.0', 'C2', 'B-2', 'G2', '7.11.2', 'G2', 'F#2', 'G3', 'C5', '4.7.11', 'B-2', 'G2', 'G2', 'F#2', 'G3', 'B-2', 'C2', 'F2', 'F2', 'F#2', 'F3', '5.9.0', 'B-2', 'F2', '2.5.9', 'F2', 'F#2', 'F3', 'C5', '4.8.11', 'C2', 'B-2', 'E2', 'E2', 'F#2', 'E3', 'D2', 'E2', 'D2', 'E2', 'D2', 'D2', 'E3', 'D2', 'C2', 'B-2', 'A2', 'A2', 'C2', 'F#2', 'A3', '9.0.4', 'C2', 'B-2', 'A2', '9.2', 'A2', 'C2', 'C2', 'F#2', 'A3', '9.0', 'C2', 'C2', 'B-2', 'G2', '7.11.2', 'G2', 'C2', 'F#2', 'G3', 'C5', '4.7.11', 'B-2', 'C2', 'G2', 'G2', '4.7.11', 'C2', 'F#2', 'C2', 'G3', 'C2', 'C2', 'B-2', 'F2', 'F2', 'F#2', 'C2', 'F3', '5.9.0', 'C2', 'B-2', 'C2', 'F2', '2.5.9', 'F2', 'C2', 'C2', 'F#2', 'F3', 'C5', '4.8.11', 'C2', 'C2', 'B-2', 'E2', 'E2', 'C2', 'C2', 'F#2', 'E3', 'C2', 'B-2', 'E2', 'E2', 'C2', 'C2', 'F#2', 'E3', 'C2', 'B-2', 'C2', 'A2', 'A2', 'C2', 'F#2', 'C2', 'A3', '9.0.4', 'C2', 'C2', 'B-2', 'A2', '9.2', 'A2', '9.0', 'C2', 'F#2', 'C2', 'A3', '7.11.2', 'C2', 'B-2', 'C2', 'G2', 'G2', '7.0', 'C2', 'F#2', 'G3', '4.7.11', 'C2', 'B-2', 'G2', 'G2', '4.7.11', 'C2', 'F#2', 'C2', 'G3', 'C2', 'B-2', 'C2', 'F2', 'F2', 'C2', 'F#2', 'C2', 'F3', '5.9.0', '2.5.9', 'C2', 'B-2', 'C2', 'F2', 'F2', '5.9.0', 'C2', 'F#2', 'C2', 'F3', '4.8.11', 'C2', 'C2', 'B-2', 'E2', 'E2', 'D2', 'D2', 'E3', 'D2', 'D2', 'B-2', 'C2', 'E2', 'E2', 'D2', 'D2', 'E3', 'D2', 'D2', 'C2', 'B-2', 'A2', '4.9', 'F#2', 'C2', 'A2', 'E5', 'E5', 'B-2', 'C2', 'A2', 'G5', 'C2', 'C2', 'F#2', 'A2', 'C2', 'C2', 'B-2', '7.0', 'C3', 'A5', 'G5', 'C2', 'F#2', 'C3', 'E5', 'B-2', 'C2', 'C3', 'C2', 'C2', 'F#2', 'C3', 'C5', 'C2', 'C2', 'B-2', '2.6.9', 'D3', 'C2', 'F#2', 'D3', 'D5', 'D5', 'B-2', 'C2', 'D3', 'C5', 'C2', 'C2', 'F#2', 'D3', 'E5', 'C2', 'B-2', 'C2', 'D3', 'A4', 'F#2', 'C2', 'D3', 'B-2', 'C2', 'D3', 'C2', 'C2', 'F#2', 'D3', 'C2', 'C2', 'B-2', 'A2', '4.9', 'C2', 'F#2', 'A2', 'E5', 'B-2', 'C2', 'A2', 'E5', 'C2', 'C2', 'F#2', 'A2', 'G5', 'A5', 'C2', 'C2', 'B-2', '7.0', 'C3', 'G5', 'C2', 'C2', 'F#2', 'C3', 'E5', 'C2', 'B-2', 'C2', 'C3', 'D5', 'C2', 'C2', 'F#2', 'C3', 'C5', 'C2', 'C2', 'B-2', '2.6.9', 'D3', 'C2', 'F#2', 'C2', 'D3', 'D5', 'C2', 'C2', 'B-2', 'D3', 'D5', 'C5', 'C2', 'F#2', 'C2', 'D3', 'E5', 'C2', 'C2', 'B-2', 'D3', 'C2', 'F#2', 'C2', 'D3', 'A4', 'C5', 'C2', 'C2', 'B-2', 'D3', 'D5', 'C2', 'F#2', 'C2', 'D3', 'C2', 'C2', 'B-2', 'A2', '4.9', 'C2', 'F#2', 'A2', 'E5', 'B-2', 'C2', 'A2', 'D5', 'C2', 'F#2', 'C2', 'A2', 'C5', 'E5', 'C2', 'B-2', 'C2', '7.0', 'C3', 'E5', 'F#2', 'C2', 'C3', 'B-2', 'C2', 'C3', 'C2', 'F#2', 'C2', 'C3', 'C2', 'C2', 'B-2', '2.6.9', 'D3', 'C2', 'F#2', 'D3', 'D5', 'B-2', 'C2', 'D3', 'D5', 'C2', 'F#2', 'C2', 'D3', 'C5', 'E5', 'C2', 'C2', 'B-2', 'D3', 'C2', 'F#2', 'D3', 'C2', 'B-2', 'D3', 'C2', 'F#2', 'C2', 'D3', 'C2', 'A2', '4.9', 'C2', 'A2', 'E5', 'C2', 'B-2', 'A2', 'D5', 'C2', 'A2', 'C5', 'E5', '7.0', '7.0', 'C3', 'C3', 'C3', 'F#2', 'D2', 'D2', 'D2', 'C3', 'D2', 'D2', 'D2', '7.0', '2.6.9', 'D3', 'D5', 'D3', 'D5', '7.0', 'D3', 'D5', 'D3', 'C5', 'E5', 'D3', 'D3', 'D3', 'D3']\n"
     ]
    }
   ],
   "source": [
    "data, targets = get_data_from_dir(\"test_midi_small/\")\n",
    "data_dict = Seq2Seq_Dictionary(data)\n",
    "target_dict = Seq2Seq_Dictionary(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transform the data and the targets\n",
    "transformed_data = [data_dict.transform_sentence(i) for i in data]\n",
    "transformed_targets = [target_dict.transform_sentence(i) for i in targets]\n",
    "\n",
    "## Sanity check for the lengths of the data and the targets\n",
    "assert np.mean([len(x) for x in transformed_data])==data_dict.max_length\n",
    "assert np.mean([len(x) for x in transformed_targets])==target_dict.max_length\n",
    "assert len(transformed_data) == len(transformed_targets)\n",
    "\n",
    "len(data_dict.index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data vocabulary: 274\n",
      "Size of targets vocabulary: 107\n",
      "Max. Length of the data: 403\n",
      "Max. Length of the target: 2925\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-56aff2432403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Max. Length of the data: {data_dict.max_length}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Max. Length of the target: {target_dict.max_length}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sample data: {data[2]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Corresponding targets: {targets[5]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sample transformed data: {transformed_data[2]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(f\"Size of data vocabulary: {data_dict.vocab_size}\")\n",
    "print(f\"Size of targets vocabulary: {target_dict.vocab_size}\")\n",
    "\n",
    "print(f\"Max. Length of the data: {data_dict.max_length}\")\n",
    "print(f\"Max. Length of the target: {target_dict.max_length}\")\n",
    "print(f\"Sample data: {data[2]}\")\n",
    "print(f\"Corresponding targets: {targets[5]}\")\n",
    "print(f\"Sample transformed data: {transformed_data[2]}\")\n",
    "print(f\"Corresponding transformed targets: {transformed_targets[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "Tensorflow initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "START = 1\n",
    "END = 2\n",
    "\n",
    "embedding_size= 256\n",
    "hidden_units = 128\n",
    "keep_prob=0.5 # Dropout parameter\n",
    "batch_size = 64\n",
    "max_seq_length = data_dict.max_length\n",
    "vocab_size = len(data_dict.index2word_map)\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_encoder_inputs = tf.placeholder(shape=(batch_size, max_seq_length),\n",
    "                                 dtype=tf.int32, name='encoder_inputs')\n",
    "_encoder_seq_len = tf.placeholder(shape=(batch_size),\n",
    "                                 dtype=tf.int32, name='encoder_seq_lens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder part is created here. In the architecture, a bidirectional GRU cell is used after the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "_encoder_inputs = tf.placeholder(shape=(batch_size, max_seq_length),\n",
    "                                 dtype=tf.int32, name='encoder_inputs')\n",
    "_encoder_seq_len = tf.placeholder(shape=(batch_size),\n",
    "                                 dtype=tf.int32, name='encoder_seq_lens')\n",
    "_decoder_inputs = tf.placeholder(shape=(batch_size,max_seq_length),\n",
    "                                 dtype=tf.int32, name='decoder_inputs')\n",
    "### remove before here\n",
    "with tf.variable_scope(\"encoder\") as encoder_sc:\n",
    "    ## embeddings\n",
    "    enc_embed_var = tf.Variable(\n",
    "        tf.random_uniform([vocab_size,\n",
    "                           embedding_size],\n",
    "                          -1.0, 1.0), name='embedding')\n",
    "    \n",
    "    enc_embed = tf.nn.embedding_lookup(enc_embed_var, _encoder_inputs)\n",
    "    \n",
    "    # Forward direction cell\n",
    "    enc_gru_fw = tf.nn.rnn_cell.GRUCell(hidden_units)\n",
    "    # Backward direction cell\n",
    "    enc_gru_bw = tf.nn.rnn_cell.GRUCell(hidden_units)\n",
    "    \n",
    "    enc_dropout_fw = tf.contrib.rnn.DropoutWrapper(enc_gru_fw, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "\n",
    "    enc_dropout_bw = tf.contrib.rnn.DropoutWrapper(enc_gru_bw, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "\n",
    "    \n",
    "    ## here the state variable contains only the last state information of the cells\n",
    "    enc_rnn_outputs,enc_rnn_state=tf.nn.bidirectional_dynamic_rnn(enc_dropout_fw,\n",
    "                                                          enc_dropout_bw, \n",
    "                                                          enc_embed,\n",
    "                                                          sequence_length=_encoder_seq_len,\n",
    "                                                          dtype=tf.float32)\n",
    "    ## Get forward and backward last states and outputs of the GRU\n",
    "    enc_rnn_outputs_fw,enc_rnn_outputs_bw  = enc_rnn_outputs\n",
    "    enc_rnn_fw_state,enc_rnn_bw_state  = enc_rnn_state\n",
    "    \n",
    "    ## concat states and outputs\n",
    "    _enc_last_state = tf.concat((enc_rnn_bw_state, enc_rnn_fw_state),1)\n",
    "    _enc_output = tf.concat((enc_rnn_outputs_bw,enc_rnn_outputs_fw),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_enc_last_state.get_shape())\n",
    "print(_enc_output.get_shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder part is created here. Because bidirectional GRU  is used in the encoder part the state vector is twice size of an GRU cell with same number of hidden units. So, after concatanating the last states of GRUs, here the hidden units of GRU should be doubled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder  With While Loop\n",
    "We are using a while loop structure because each resulting hidden state of the GRU in the decoder, will be an input to the network to calculate scores of the next word in the same sentence.\n",
    "\n",
    "Following is the condition for the while loop. From the first word at each sentence, iteration should go until the last word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decoder_condition(t, *args):\n",
    "    return t<max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder as a function to be called from the body of the while_loop. Note that, in order to reuse the network after each word, first we need to initialize it then set the reuse to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#_enc_last_state = tf.placeholder(shape=(batch_size, 2*hidden_units),\n",
    "#                                 dtype=tf.float32, name='decoder_input_enc_last_state')\n",
    "#_enc_output = tf.placeholder(shape=(batch_size,max_seq_length ,2*hidden_units),\n",
    "#                                 dtype=tf.float32, name='decoder_input_enc_last_state')\n",
    "#_decoder_inputs = tf.placeholder(shape=(batch_size),\n",
    "#                                 dtype=tf.int32, name='decoder_inputs')\n",
    "\n",
    "def decoder(_decoder_inputs,_hidden_state,reuse=None):\n",
    "    with tf.variable_scope(\"decoder\",reuse=reuse) as decoder_sc:\n",
    "        ## Luong's multiplicative score --> score = _hidden_state.T * W * _enc_output\n",
    "\n",
    "        ### First the W*_enc_output part is handled. It is straightforward with a dense layer, \n",
    "        ### and its output size should be hidden_size*2, because we have a bidirectional rnn \n",
    "        ### in the encoder. Output shape should be (batch_size, max_len, 2*hidden_size)\n",
    "        ### because later it will be multiplied with (batch_size,2*hidden_size) (which could be thought\n",
    "        ### as batch_size, 2*hidden_size, 1) to get the score.\n",
    "        w_times_enc_output = tf.layers.dense(_enc_output, hidden_units*2)\n",
    "        print(\"shape of w_times_enc_output:\",w_times_enc_output.get_shape())\n",
    "\n",
    "        ### First hidden state is taken from the encoder's GRUs last hidden state. So the\n",
    "        ### shape of it is (batch_size, 2*hidden_size). For each input sentence, there is one\n",
    "        ### hidden state.\n",
    "        ### _hidden_state's size is (batch_size, 2*hidden_size) one can think of it as \n",
    "        ### (batch_size, 1,2*hidden_size). Semantically, there is only one hidden state vector\n",
    "        ### for each batch item(iteration).To transpose it, as the formula of Luong's suggests,\n",
    "        ### we can just expand (batch_size, 2*hidden_size) to (batch_size, 2*hidden_size,1), \n",
    "        ### expanding in the 2.nd dimension.\n",
    "        hidden_state_tr = tf.expand_dims(_hidden_state,2)\n",
    "        print(\"shape of enc_last_state_tr:\",hidden_state_tr.get_shape())\n",
    "\n",
    "        ### w_times_enc_output = (batch_size, max_len, 2*hidden_size)\n",
    "        ### enc_last_state_tr = (batch_size, 2*hidden_size,1)\n",
    "        ### resulting score = (batch_size, max_len,1)\n",
    "        score =  tf.matmul(w_times_enc_output,hidden_state_tr)\n",
    "        print(\"shape of score:\",score.get_shape())\n",
    "\n",
    "        ### Now the shape of score (batch_size, max_len,1). We have a score for each of the \n",
    "        ### input word in a bacth. To normalize it, now they are put in a softmax, and \n",
    "        ### the normalization should be within a batch, so the axis to apply softmax is\n",
    "        ### 1.st one, since 0 is used for batches.\n",
    "        ### Attention weights(attention_w) has same shape with score, which is (batch_size, max_len,1)\n",
    "        attention_w = tf.nn.softmax(score,1)\n",
    "\n",
    "        ### attention_w (batch_size, max_len,1),   _enc_output (batch_size, max_len,2*hidden_size).\n",
    "        ### Multiplication operator supports broadcasting, so that this multiplication does not produce\n",
    "        ### an error. attention_w is broadcasted to be multiplied with each hidden unit of _enc_output,.\n",
    "        ### which means multiplying each output of the hidden units with the attention weight of the\n",
    "        ### associated word.\n",
    "        ### Resulting context_vec is in shape of (batch_size, max_len, 2*hidden_size)\n",
    "        context_vec = attention_w * _enc_output\n",
    "\n",
    "        ### To create a context vector for each sentence in the batch, now we are summing\n",
    "        ### up along the dimension of the max_len(along words in a sentence) \n",
    "        ### so that we are left with size (batch_size, 2*hidden_size).\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "        print(\"shape of context_vec:\",context_vec.get_shape())\n",
    "\n",
    "        ### Input to the decoder is also put through a embedding layer, since they are\n",
    "        ### target sentences.\n",
    "        embed_var = lambda: tf.random_uniform([vocab_size,embedding_size],-1.0, 1.0)\n",
    "        dec_embed_var = tf.Variable(embed_var ,name='decoder_embedding')\n",
    "\n",
    "        ### Size of the embedded input-> (batch_size, 1, embedding_size)\n",
    "        dec_embed = tf.nn.embedding_lookup(dec_embed_var, tf.expand_dims(_decoder_inputs,1))\n",
    "\n",
    "        print(\"shape of the decoder embedding:\",dec_embed.get_shape())\n",
    "\n",
    "        ### To make the 1.st dimension matching with the embedded input, now the context vector \n",
    "        ### is expanded in the 1.st dimension. resulting size is (batch_size, 1, 2*hidden_size)\n",
    "        context_vec = tf.expand_dims(context_vec, 1)\n",
    "\n",
    "        ### Concatanate along the second dimension, so the resulting size is\n",
    "        ### (batch_size, 1, 2*hidden_size + hidden_dim)\n",
    "        dec_before_gru = tf.concat([context_vec, dec_embed], axis=2)\n",
    "\n",
    "        ### Since we will be feeding the decoder one input at a time, the sequence length\n",
    "        ### would be either 0 or 1 depending on the current input of each sentence.\n",
    "        ### So if the current input is not <PAD>, then the seq len is 1, if it is <PAD> then \n",
    "        ### it is just a padding, the seq len is 0.\n",
    "        all_pads = [data_dict.get_index(\"<PAD>\")]*batch_size\n",
    "        ones = np.ones((batch_size))\n",
    "        zeros = np.zeros((batch_size))\n",
    "        dec_seq_len = tf.cast(tf.where(_decoder_inputs == all_pads, zeros, ones),\n",
    "                              dtype=tf.float32)\n",
    "\n",
    "        ### Now the input is ready for the GRU.\n",
    "        dec_gru = tf.nn.rnn_cell.GRUCell(2*hidden_units)\n",
    "\n",
    "        dec_dropout = tf.contrib.rnn.DropoutWrapper(dec_gru, input_keep_prob=keep_prob,\n",
    "                                                       output_keep_prob=keep_prob)\n",
    "\n",
    "        ### dec_rnn_outputs has shape (batch_size, 1, 2*hidden_size)\n",
    "        ### dec_rnn_state has shape (batch_size, 2*hidden_size)\n",
    "        dec_rnn_outputs,dec_rnn_state=tf.nn.dynamic_rnn(cell=dec_dropout, inputs=dec_before_gru, \n",
    "                                                        initial_state=_hidden_state,\n",
    "                                                        sequence_length=dec_seq_len)\n",
    "        ### To make predictions based on the output of the rnn, now we are reshaping the \n",
    "        ### the output to the shape of (batch_size, 2*hidden_size)\n",
    "        dec_rnn_outputs = tf.squeeze(dec_rnn_outputs)\n",
    "\n",
    "        ### predictions has the shape of (batch_size, vocab_size). This means we are predicting\n",
    "        ### only the next word for each sentence. For each sentence, there is a vector of\n",
    "        ### shape vocab_size which contains the likelihood of the corresponding vocabulary\n",
    "        ### element for the next word in the sentence.\n",
    "        preds = tf.layers.dense(dec_rnn_outputs, vocab_size)\n",
    "    \n",
    "    return preds, dec_rnn_state,dec_seq_len  #,dec_embed_var.read_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.variable_scope(\"pred_layer\") as pred_layer_sc:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the decoder inputs. \n",
    "To reduce the memory usage during the backpropagation, we are putting the each word in each sentences in a batch into a TensorArray.\n",
    "\n",
    "The size of the decoder inputs is the (batch_size, max_notes_len), aproximately the target notes of the current input batch. First we are transposing it to word major order, so that the [i,] indexing will return the i.th note of the all note sequences in the batch. Then we are going to unstack it, hence we will get a tensorarray of size max_notes_len, each item containing batch_size notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_tr = tf.transpose(_decoder_inputs)\n",
    "decoder_input_arr = tf.TensorArray(dtype=tf.int32, size=max_seq_length)\n",
    "decoder_input_arr = decoder_input_arr.unstack(decoder_inputs_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General variables to run the decoder loop. \n",
    "\n",
    "Iteration starts from 1, because we are going to call the decoder to initialize it at the beginning. \n",
    "\n",
    "init_outputs stores the output predictions of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_i = tf.constant(1, dtype=tf.int32)\n",
    "init_outputs = tf.TensorArray(dtype=tf.float32,size=max_seq_length)\n",
    "init_seq_len = tf.TensorArray(dtype=tf.init32,size=max_seq_length)\n",
    "#init_embed_vals = tf.random_uniform([vocab_size,embedding_size],-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the decoder to be able to \"reuse\" it, note that reuse is None as default. Initial hidden state is from the encoder and first decoder input is the 0.th element of the decoder_input_arr. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_preds,init_hidden_state,temp_seq_len = decoder(decoder_input_arr.read(0), _enc_last_state)\n",
    "                                       #init_embed_vals)\n",
    "init_outputs = init_outputs.write(0, init_preds)\n",
    "init_seq_len = init_seq_len.write(0, temp_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the decoder with the while loop. We need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_body(iteration,outputs,body_hidden_state,seq_len):\n",
    "    temp_preds,temp_hid_state,temp_seq_len = decoder(\n",
    "                                    _decoder_inputs=decoder_input_arr.read(iteration), \n",
    "                                    _hidden_state=body_hidden_state,\n",
    "                                    #_embedding_var = embed_val,\n",
    "                                    reuse=True)\n",
    "    outputs = outputs.write(iteration, temp_preds)\n",
    "    seq_len = seq_len.write(iteration, temp_seq_len)\n",
    "    return iteration+1, outputs, temp_hid_state       ##,temp_embed_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally run the while loop. We dont need the latest hidden state and the iteration count, the only thing needed is the predictions of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dec_preds, _ = tf.while_loop(decoder_condition, decoder_body, \n",
    "                                [init_i, init_outputs, init_hidden_state,init_seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, dec_preds contains the predictions for each word. So each item in the tensorarray contains (batch_size, vocab_size) shaped tensors. \n",
    "\n",
    "Now we are going to create a one tensor whose shape is (batch, max_len,vocab_size). To achieve it, we are going to stack the dec_preds, which results in (max_notes_len, batch_size, vocab_size). Then transpose to get (batch_size,max_notes_len,vocab_size). And the same is done for the seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.transpose(dec_preds.stack(), [1,0,2])\n",
    "all_seq_len = tf.transpose(seq_len.stack(), [1,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and the Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the mask for the backpropagation. If the target is <PAD> then don't backpropagate. To do it, first get all <PAD> strings as 1, and then subtract from 1 to make them all 0 s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(targets, preds):\n",
    "    mask=1-np.equal(targets, data_dict.get_index(\"<PAD>\"))\n",
    "    ### If the input word is <PAD>, then there is no need for optimization for that input.\n",
    "    cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds, labels=_targets) * mask\n",
    "\n",
    "    ### mean of the cross entropy is the loss of this batch\n",
    "    loss = tf.reduce_mean(cross_ent)\n",
    "    return loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Seminar",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
