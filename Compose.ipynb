{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Through Lyrics\n",
    "This is the submission of Özgür Akyazı (03693664) and Sameh Metias (03695231) for the Deep Learning for NLP Seminar at TUM, under the supervision of Prof. Dr. Simon Hegelich.\n",
    "\n",
    "All files could also be found here https://gitlab.com/xoxgur123/seq2seq-compose-by-lyrics\n",
    "\n",
    "The purpose of this project is to learn the relation between the lyrics of a song and its music. It is certain that different kind of lyrics require some different kind of music that indicate the connotation behind the used words. From a technical point of view, we have two sequences that we are trying to relate:\n",
    "\n",
    "1) The first sequence is a list of words of the lyrics of the song.\n",
    "\n",
    "2) The second sequence is a list of notes/chords that compose the music of the song.\n",
    "\n",
    "With that being said, we built an encoder/decoder seq2seq model with GRU recurrent units to try to capture the relationship between the lyrics and the music. We followed the Neural Machine translation Model depicted in the image below and transferred its task to lyrics-to-notes translation.\n",
    "\n",
    "\n",
    "In order to acquire some training data we needed to get creative. We thought of gathering midi files of songs that already contain the lyrics and use these as training data. Although the idea is valid, but gathering the data was a huge challenge towards this project. Most of the midi files on the internet are either expensive to get (it is the same kind of files that is used for kareoke), or the files are assumed that they contain the lyrics while in reality they do not. Luckily we were able to find http://www.olgris.kiev.ua/des/midi%20lat.html, which was our main source of data. The website contains 230 midi files with all different genres of music.\n",
    "\n",
    "## Background Knowledge\n",
    "In order to be able to work with the midi files, we needed to know what a midi file is in the first place in order to look in the right place for the data required. Without going into much detail, midi files are byte-encoded files that include all different information about a piece of music. The most important building units of a midi file are the tracks. A midi file includes a track for every instrument played in that song. However there is one special track, usually that of the piano, that includes also the lyrics. Each track consists of several events of different types. Each event contains then its own metadata that is used when playing the file. There are two important types of events in every track:\n",
    "\n",
    "1) The \"NOTE\" event which is translated to a certain sound, and\n",
    "    \n",
    "2) The \"LYRIC\" event that contains the word or words at a specific position.\n",
    "    \n",
    "Events in a track are sequential data and are thus played in certain order. This is why the piano track is considered for this project. In order to be able to generate more data, we considered the sentence level of the lyrics. All notes/chords sequence associated with a sentence is extracted and treated as a training sample. Sentence are recognized through a return character, which is a pattern recognized while working on the midi files.\n",
    "\n",
    "As mentioned above, midi files are byte-encoded files, and it would have been quite a hassle to work with them as raw files. This is why we used the music21 library developed at MIT https://web.mit.edu/music21/. Before getting started, please follow the installation instructions stated below.\n",
    "\n",
    "## Installation steps\n",
    "Please install all packages listed in requirements.txt by running \"pip install -r requirements.txt\".\n",
    "In addition to that, spaCy was used for text preprocessing. This is why a spaCy model needs to be downloaded with the command \"python -m spacy download en\". Ultimately, the package timidity needs to be installed in order to play music in this notebook. Install timidity via the command \"sudo apt-get install timidity timidity-interfaces-extra\". There also have to be 2 empty directories in the project directory: logs/ and output/. The code shpuld also run using python 3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A view on the data\n",
    "Thanks to music21 and timidity, we can play midi files from a notebook. Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preprocess import *\n",
    "import os,glob\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file = \"test_midi/14_Years.mid\"\n",
    "play_midi(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lyrics of the song is included also in the midi file. If we were to play it in a terminal using \"timidity -filename-\", we would get the lyrics as in a kareoke game. Because the piano track is the only track of interest to us, we need to look at the events in order to know the type of the data we are dealing with. Here are 10 events from that track: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = midi.MidiFile()\n",
    "m.open(file)\n",
    "m.read()\n",
    "for track in m.tracks:\n",
    "    lyrics = [ev.data for ev in track.events if ev.type==\"LYRIC\"]\n",
    "    temp_stream = midi.translate.midiTrackToStream(track)\n",
    "    notes = get_notes_from_stream(temp_stream)\n",
    "    if len(lyrics) > 0:\n",
    "        break\n",
    "print(track.events[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these events, we came up with a string representation that would allow us to convert the strings back to notes/chords. The string representation of 20 notes/chords from this track are shown below. As we can see, we convert notes by mapping them to their pitches. As for chords, a string of the normal order representation of each note in the chord separated by a dot is recorded. For the detailed operation, please refer to the \"get_notes_from_stream\" in preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notes[110:130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the data is ready and can be fed to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special words for the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_words = [\"<PAD>\", \"<GO>\", \"<END>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class For Dictionary\n",
    "The class contains the data and the helper functions for the dictionary to train seq2seq model.\n",
    "Most important functions are to mapping a list of words(a sentence) to a list of corresponding integer(indeces of words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Dictionary:\n",
    "    def __init__(self, sentences ):\n",
    "        self.word2index_map = dict()\n",
    "        self.index2word_map = dict()\n",
    "        self.vocab_size = 0\n",
    "        self.init_register(sentences)\n",
    "        \n",
    "    # Initiates word2index_map and index2word_map\n",
    "    # Also extracts the max number of words in the sentences and saves it \n",
    "    def init_register(self,sentences):\n",
    "        global special_words\n",
    "        current_index = 0\n",
    "        ## save the maximum length among the sentences. \n",
    "        self.max_length = max([len(sentence) for sentence in sentences])\n",
    "        ### map special words, initially the mappings are empty.\n",
    "        for word in special_words:\n",
    "            self.word2index_map[word] = current_index\n",
    "            self.index2word_map[current_index] = word\n",
    "            current_index+=1\n",
    "        \n",
    "        s = set([item for sublist in sentences for item in sublist])\n",
    "        self.word2index_map.update({e:i+current_index for i,e in enumerate(s)})\n",
    "        self.index2word_map.update({v:k for k,v in self.word2index_map.items()})\n",
    "        self.vocab_size = len(self.index2word_map)\n",
    "    \n",
    "    ## Returns the index of the word in the dictionary. It is assumed that the word\n",
    "    ## will be always in dictionary.\n",
    "    def get_index(self, word):\n",
    "        return self.word2index_map[word]\n",
    "    \n",
    "    ## Maps a sentence, which is a list of words, to the corresponding list of integers.\n",
    "    ## Each word is looked up from the map of the dictionary, and as in get_index method,\n",
    "    ## it is assumed that the word will always be found in the dictionary\n",
    "    def map_sentence(self, sentence):\n",
    "        return [self.get_index(i) if i in self.word2index_map else 0 for i in sentence]\n",
    "    \n",
    "    ## Returns the word by its index in dictionary.\n",
    "    def get_word(self, index):\n",
    "        return self.index2word_map[word]\n",
    "    \n",
    "    ## Pads the list of words to <PAD> at the end of the list of words in sentence\n",
    "    def pad_sentence(self, sentence):\n",
    "        return  sentence + [\"<PAD>\"] * (self.max_length - len(sentence)+2)\n",
    "    \n",
    "    ## Adds <GO> and <END> to the start and end of the sentence\n",
    "    def add_start_end_tokens(self, sentence):\n",
    "        return [\"<GO>\"] + sentence + [\"<END>\"]\n",
    "    \n",
    "    ## Transforms the sentence in a format suitable for the Neural Network\n",
    "    def transform_sentence(self, sentence):\n",
    "        s = self.add_start_end_tokens(sentence)\n",
    "        s = self.pad_sentence(s)\n",
    "        s = self.map_sentence(s)\n",
    "        return s\n",
    "    \n",
    "    def reverse_transform(self, sentence):\n",
    "        return [self.index2word_map[s] for s in sentence if (not self.index2word_map[s] in special_words and s in self.index2word_map)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Music Files\n",
    "The variable data conytains the lyrics and  the variable targets contains the target notes/notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change directory to \"test_midi_medium/\" or \"test_midi/\" for larger datasets\n",
    "data, targets = get_data_from_dir(\"test_midi_small/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input and target dictionaries, and general useful variables. max_len(max_sequence_len and max_notes_len) variables are the sizes of the longest sentences for lyrics and notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = Seq2Seq_Dictionary(data)\n",
    "target_dict = Seq2Seq_Dictionary(targets)\n",
    "max_sentence_len = data_dict.max_length\n",
    "max_notes_len = target_dict.max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dictionary, we need to transform both lyrics and notes by adding the < GO >, < END > and < PAD > words, as necessary and, get the integer representations of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the data and the targets\n",
    "transformed_data = [data_dict.transform_sentence(i) for i in data]\n",
    "transformed_targets = [target_dict.transform_sentence(i) for i in targets]\n",
    "print(data_dict.max_length)\n",
    "print(target_dict.max_length)\n",
    "print(data[10])\n",
    "print(transformed_data[10])\n",
    "len(data_dict.index2word_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the grasp of the data we are dealing, here are some logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of data vocabulary: {data_dict.vocab_size}\")\n",
    "print(f\"Size of targets vocabulary: {target_dict.vocab_size}\")\n",
    "\n",
    "print(f\"Max. Length of the data: {data_dict.max_length}\")\n",
    "print(f\"Max. Length of the target: {target_dict.max_length}\")\n",
    "print(f\"Sample data: {data[10]}\")\n",
    "print(f\"Corresponding targets: {targets[2]}\")\n",
    "print(f\"Sample transformed data: {transformed_data[10]}\")\n",
    "print(f\"Corresponding transformed targets: {transformed_targets[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "Tensorflow initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size= 256\n",
    "hidden_units = 256\n",
    "keep_prob=0.5 # Dropout parameter\n",
    "batch_size = 32\n",
    "sentence_vocab_size = len(data_dict.index2word_map)\n",
    "notes_vocab_size = len(target_dict.index2word_map)\n",
    "learning_rate = 1e-4\n",
    "SAVE_PATH = 'logs/Compose/' ### Save the trained model\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.mkdir(SAVE_PATH)\n",
    "    \n",
    "test_train_ratio = 0.2  ### ratio of the data to be allocated as test(actually validation) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Inputs\n",
    "\\_encoder\\_inputs has size of max_sentence_len+2 because we have extra < GO > and < END > variables in the input.\n",
    "\n",
    "To differentiate between train and test time, which changes the graph in our model, \\_is\\_training input is presented.\n",
    "\n",
    "\\_target\\_notes has column size of max\\_notes\\_len+1, contrary to \\_encoder_inputs, because the < GO > word, at the beginning of the decoder is given by default, and actually the decoder does not try to predict the < GO > word. It is the first input to the decoder, but the first output of the decoder is actually the first note/chord in the notes sequence. So, we need to exclude < GO > from the \\_target_notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_encoder_inputs = tf.placeholder(shape=(batch_size, max_sentence_len+2),\n",
    "                                 dtype=tf.int32, name='encoder_inputs')\n",
    "_encoder_seq_len = tf.placeholder(shape=(batch_size),\n",
    "                                 dtype=tf.int32, name='encoder_seq_lens')\n",
    "_is_training = tf.placeholder(tf.bool,name=\"training_or_test\")\n",
    "_target_notes = tf.placeholder(shape=(batch_size, max_notes_len+1) , \n",
    "                               dtype=tf.int32, name='target_notes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and The Encoder\n",
    "Encoder part is created here. First the words in the lyrics are put in an embedding layer. Then the a bidirectional GRU cell is used after the embedding, which contains forward and backward GRU cells, and after putting the into the dropout layers, they are put in bidirectional_dynamic_rnn, with sequence length is defined with the input \\_encoder_seq_len. \n",
    "\n",
    "Then the output and the hidden states of it is taken. We have output and a hidden state for both forward and backward GRUs. To use them in the decoder, we concatenate hidden states and outputs of each. The hidden state will be used as the initial hidden state of the GRU in the decoder, and outputs will be used to create scores. The following image would summarize what we are doing with encoder and decoder.\n",
    "\n",
    "![title](model.png)\n",
    "\n",
    "The main idea is to run the encoder for all the input in a batch at once. Then, predict the notes/chords one by one. Because the output after predicting a note will be used when predicting the next note. That is actually the reason why we are using a while loop for the decoder part. The detailed comments about the structure could be found in the inline comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"encoder\") as encoder_sc:\n",
    "    ## embeddings\n",
    "    enc_embed_var = tf.Variable(\n",
    "        tf.random_uniform([sentence_vocab_size,\n",
    "                           embedding_size],\n",
    "                          -1.0, 1.0), name='embedding')\n",
    "    \n",
    "    enc_embed = tf.nn.embedding_lookup(enc_embed_var, _encoder_inputs)\n",
    "    \n",
    "    # Forward direction cell\n",
    "    enc_gru_fw = tf.nn.rnn_cell.GRUCell(hidden_units)\n",
    "    # Backward direction cell\n",
    "    enc_gru_bw = tf.nn.rnn_cell.GRUCell(hidden_units)\n",
    "    \n",
    "    enc_dropout_fw = tf.contrib.rnn.DropoutWrapper(enc_gru_fw, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "\n",
    "    enc_dropout_bw = tf.contrib.rnn.DropoutWrapper(enc_gru_bw, input_keep_prob=keep_prob,\n",
    "                                                   output_keep_prob=keep_prob)\n",
    "\n",
    "    \n",
    "    ## here the state variable contains only the last state information of the cells\n",
    "    enc_rnn_outputs,enc_rnn_state=tf.nn.bidirectional_dynamic_rnn(enc_dropout_fw,\n",
    "                                                          enc_dropout_bw, \n",
    "                                                          enc_embed,\n",
    "                                                          sequence_length=_encoder_seq_len,\n",
    "                                                          dtype=tf.float32)\n",
    "    ## Get forward and backward last states and outputs of the GRU\n",
    "    enc_rnn_outputs_fw,enc_rnn_outputs_bw  = enc_rnn_outputs\n",
    "    enc_rnn_fw_state,enc_rnn_bw_state  = enc_rnn_state\n",
    "    \n",
    "    ## concat states and outputs\n",
    "    ## size of the enc_rnn_bw_state and enc_rnn_fw_state is \n",
    "    ## (batch_size, hidden_units). We are concatenating them in the first\n",
    "    ## axis to get size of (batch_size, 2*hidden_units), to be used as the initial \n",
    "    ## hidden state of the decoder.\n",
    "    ## And the size of the enc_rnn_outputs_bw and enc_rnn_outputs_fw is \n",
    "    ## (batch_size, max_sentence_len+2, hidden_units). They are concatenated in the 2.nd axis\n",
    "    ## because each words output is in the second axis, decoded with hidden_units number of \n",
    "    ## numbers. \n",
    "    _enc_last_state = tf.concat((enc_rnn_bw_state, enc_rnn_fw_state),1)\n",
    "    _enc_output = tf.concat((enc_rnn_outputs_bw,enc_rnn_outputs_fw),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_enc_last_state.get_shape())\n",
    "print(_enc_output.get_shape())\n",
    "print(enc_rnn_outputs_bw.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "Decoder part is created here. Because bidirectional GRU  is used in the encoder part, the state vector is twice size of an GRU cell with same number of hidden units. So, after concatanating the last states of GRUs, here the hidden size of GRU should be doubled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder  With While Loop\n",
    "We are using a while loop structure because each resulting hidden state of the GRU in the decoder, will be an input to the network to calculate scores of the next word in the same sentence.\n",
    "\n",
    "Following is the condition for the while loop. As this returns true, the while_loop will be running and giving the next word. As the < GO > word is given as default input, and considering the variable t (actually init_i varaible, defined later) starts from 1, the read from the related varaible( decoder_input_arr) is done with t-1. From the first word at each sentence, iteration should go until the last note/chord, except the < END >. Since the < END > word is not an input the decoder, we should not read it, it is only the output of the decoder, a word to be predicted. Hence, the iteratation goes from beginning to the end, but the < END > is not read, since the read is done with t-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_condition(t, *args):\n",
    "    return t<max_notes_len+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder as a function to be called from the body of the while_loop. Note that, in order to reuse the network after each word, first we need to initialize it then set the reuse to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((batch_size))\n",
    "zeros = np.zeros((batch_size))\n",
    "def decoder(_decoder_inputs,_hidden_state,reuse=None):\n",
    "    with tf.variable_scope(\"decoder\",reuse=reuse) as decoder_sc:\n",
    "        ## Luong's multiplicative score --> score = _hidden_state.T * W * _enc_output\n",
    "\n",
    "        ### First the W * _enc_output part is handled. It is straightforward with a dense layer, \n",
    "        ### and its output size should be hidden_units*2, because we have a bidirectional rnn \n",
    "        ### in the encoder. Output shape should be \n",
    "        ### (batch_size, max_sentence_len+2, 2*hidden_units)\n",
    "        ### because later it will be multiplied with (batch_size,2*hidden_units) \n",
    "        ### (which could be thought as batch_size, 2*hidden_units,1) to get the score.\n",
    "        w_times_enc_output = tf.layers.dense(_enc_output, hidden_units*2)\n",
    "        print(\"shape of w_times_enc_output:\",w_times_enc_output.get_shape())\n",
    "\n",
    "        ### First hidden state is taken from the encoder's GRUs last hidden state, and the\n",
    "        ### shape of it is (batch_size, 2*hidden_units). For each input sentence, there is one\n",
    "        ### hidden state. _hidden_state's size is (batch_size, 2*hidden_units) \n",
    "        ### one can think of it as (batch_size, 1,2*hidden_units). Semantically, there is \n",
    "        ### only one hidden state vector for each batch item(iteration).\n",
    "        ### To transpose it, as the formula of Luong's suggests,\n",
    "        ### we can just expand (batch_size, 2*hidden_units) to (batch_size, 2*hidden_units,1), \n",
    "        ### expanding in the 2.nd dimension.\n",
    "        hidden_state_tr = tf.expand_dims(_hidden_state,2)\n",
    "        print(\"shape of enc_last_state_tr:\",hidden_state_tr.get_shape())\n",
    "\n",
    "        ### w_times_enc_output = (batch_size, max_sentence_len+2, 2*hidden_units)\n",
    "        ### enc_last_state_tr = (batch_size, 2*hidden_units,1)\n",
    "        ### resulting score = (batch_size, max_sentence_len+2,1)\n",
    "        score =  tf.matmul(w_times_enc_output,hidden_state_tr)\n",
    "        print(\"shape of score:\",score.get_shape())\n",
    "\n",
    "        ### Now the shape of score (batch_size, max_sentence_len+2,1).\n",
    "        ### We have a score for each of the \n",
    "        ### input word(input word of the encoder) in a batch. To normalize it, now they are \n",
    "        ### put in a softmax, and \n",
    "        ### the normalization should be within a batch, so the axis to apply softmax is\n",
    "        ### 1.st one, since 0 is used for batches.\n",
    "        ### Attention weights(attention_w) has same shape with score, which is \n",
    "        ### (batch_size, max_sentence_len+2,1)\n",
    "        attention_w = tf.nn.softmax(score,1)\n",
    "\n",
    "        ### attention_w (batch_size, max_sentence_len+2,1),   \n",
    "        ### _enc_output (batch_size, max_sentence_len+2,2*hidden_size).\n",
    "        ### Multiplication operator supports broadcasting, so that this \n",
    "        ### multiplication does not produce an error. attention_w is broadcasted \n",
    "        ### to be multiplied with each hidden unit of _enc_output,\n",
    "        ### which means multiplying each output of the hidden units with the attention weight of the\n",
    "        ### associated word.\n",
    "        ### Resulting context_vec is in shape of (batch_size, max_sentence_len+2, 2*hidden_size)\n",
    "        context_vec = attention_w * _enc_output\n",
    "\n",
    "        ### To create a context vector for each sentence in the batch, now we are summing\n",
    "        ### up along the dimension of the max_len(along words in a sentence) \n",
    "        ### so that we are left with size (batch_size, 2*hidden_size).\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "        print(\"shape of context_vec:\",context_vec.get_shape())\n",
    "\n",
    "        ### Input to the decoder is also put through a embedding layer, since they are\n",
    "        ### target notes.\n",
    "        embed_var = lambda: tf.random_uniform([notes_vocab_size,embedding_size],-1.0, 1.0)\n",
    "        dec_embed_var = tf.Variable(embed_var ,name='decoder_embedding')\n",
    "\n",
    "        ### Size of the embedded input-> (batch_size, 1, embedding_size)\n",
    "        dec_embed = tf.nn.embedding_lookup(dec_embed_var, tf.expand_dims(_decoder_inputs,1))\n",
    "\n",
    "        print(\"shape of the decoder embedding:\",dec_embed.get_shape())\n",
    "\n",
    "        ### To make the 1.st dimension matching with the embedded input, now the context vector \n",
    "        ### is expanded in the 1.st dimension. resulting size is (batch_size, 1, 2*hidden_size)\n",
    "        context_vec = tf.expand_dims(context_vec, 1)\n",
    "\n",
    "        ### Concatanate along the second dimension, so the resulting size is\n",
    "        ### (batch_size, 1, 2*hidden_units + embedding_size)\n",
    "        dec_before_gru = tf.concat([context_vec, dec_embed], axis=2)\n",
    "\n",
    "        ### Since we will be feeding the decoder one input at a time, the sequence length\n",
    "        ### would be either 0 or 1 depending on the current input of each sentence.\n",
    "        ### So if the current input is not <PAD>, then the seq len is 1, if it is <PAD> then \n",
    "        ### it is just a padding, the seq len is 0.\n",
    "        all_pads = [target_dict.get_index(\"<PAD>\")]*batch_size\n",
    "        \n",
    "        dec_seq_len = tf.cast(tf.where(_decoder_inputs == all_pads, zeros, ones),\n",
    "                              dtype=tf.float32)\n",
    "\n",
    "        ### Now the input is ready for the GRU.\n",
    "        dec_gru = tf.nn.rnn_cell.GRUCell(2*hidden_units)\n",
    "\n",
    "        dec_dropout = tf.contrib.rnn.DropoutWrapper(dec_gru, input_keep_prob=keep_prob,\n",
    "                                                       output_keep_prob=keep_prob)\n",
    "\n",
    "        ### dec_rnn_outputs has shape (batch_size, 1, 2*hidden_size)\n",
    "        ### dec_rnn_state has shape (batch_size, 2*hidden_size)\n",
    "        dec_rnn_outputs,dec_rnn_state=tf.nn.dynamic_rnn(cell=dec_dropout, inputs=dec_before_gru, \n",
    "                                                        initial_state=_hidden_state,\n",
    "                                                        sequence_length=dec_seq_len)\n",
    "        ### To make predictions based on the output of the rnn, now we are reshaping the \n",
    "        ### the output to the shape of (batch_size, 2*hidden_size)\n",
    "        dec_rnn_outputs = tf.squeeze(dec_rnn_outputs)\n",
    "\n",
    "        ### predictions has the shape of (batch_size, notes_vocab_size). \n",
    "        ### This means we are predicting only the next word for each sentence. \n",
    "        ### For each sentence, there is a vector of\n",
    "        ### shape notes_vocab_size which contains the likelihood of the corresponding vocabulary\n",
    "        ### element for the next word in the sentence.\n",
    "        preds = tf.layers.dense(dec_rnn_outputs, notes_vocab_size)\n",
    "    \n",
    "    return preds, dec_rnn_state,dec_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the decoder inputs. \n",
    "To reduce the memory usage during the backpropagation, we are putting the each word in each sentences in a batch into a TensorArray.\n",
    "\n",
    "The size of the decoder inputs is the (batch_size, max_notes_len), aproximately the target notes of the current input batch. First we are transposing it to word major order, so that the [i,] indexing will return the i.th note of the all note sequences in the batch. Then we are going to unstack it, hence we will get a tensorarray of size max_notes_len, each item containing batch_size notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General variables to run the decoder loop. \n",
    "\n",
    "Iteration starts from 1, because we are going to call the decoder to initialize it at the beginning. \n",
    "\n",
    "init_outputs stores the output predictions of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_i = tf.constant(1, dtype=tf.int32)\n",
    "init_outputs = tf.TensorArray(dtype=tf.float32,size=max_notes_len+1,clear_after_read=False)\n",
    "init_seq_len = tf.TensorArray(dtype=tf.float32,size=max_notes_len+1,clear_after_read=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilize the decoder to be able to \"reuse\" it, note that reuse is None as default. Initial hidden state is from the encoder and first decoder input is all < GO > character index(which is 1) whose shape is batch_size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_preds,init_hidden_state,temp_seq_len = decoder(\n",
    "                                        [target_dict.get_index(\"<GO>\")]*(batch_size), \n",
    "                                        _enc_last_state)\n",
    "                                       #init_embed_vals)\n",
    "init_outputs = init_outputs.write(0, init_preds)\n",
    "init_seq_len = init_seq_len.write(0, temp_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using \"Teacher Forcing\" in the training time, it is a technique to provide the next input of an RNN with the label(correct output) data, instead of predicted output of the previous iteration. So that, each prediction will be starting from a correct point.\n",
    "\n",
    "The decoder will be running with the while loop. If we are using the teacher forcing, we need to use decoder_body_teacher_forcing function. But in the test time, we cannot use it, because we dont have any access to labels, so we should be using the prediction from the last word prediction of the decoder.\n",
    "\n",
    "In the decoder_body_teacher_forcing, decoder_input_arr.read(iteration-1) corresponds to the label input. Because the target_notes does not contain a < GO > word at the beginning, but contains < END > at the end, so its shape is max_notes_len+1. To read the i.th word from decoder_input_arr, knowing that iteration variable starts from 1, we need to read the input (i-1).th element from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_body_teacher_forcing(iteration,outputs,body_hidden_state,seq_len,decoder_input_arr):\n",
    "    temp_preds,temp_hid_state,temp_seq_len = decoder(\n",
    "                                    _decoder_inputs=decoder_input_arr.read(iteration-1), \n",
    "                                    _hidden_state=body_hidden_state,\n",
    "                                    #_embedding_var = embed_val,\n",
    "                                    reuse=True)\n",
    "    outputs = outputs.write(iteration, temp_preds)\n",
    "    seq_len = seq_len.write(iteration, temp_seq_len)\n",
    "    return iteration+1, outputs, temp_hid_state,seq_len,decoder_input_arr ##,temp_embed_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we need to fetch the predicted word from the previous decoder iteration. To achieve this, from outputs tensorarray, iteration-1.th element is read and with argmax, the prediction is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_body_test(iteration,outputs,body_hidden_state,seq_len,fake_decoder_input):\n",
    "    temp_preds,temp_hid_state,temp_seq_len = decoder(\n",
    "                                    _decoder_inputs=tf.argmax(outputs.read(iteration-1), axis=1), \n",
    "                                    _hidden_state=body_hidden_state,\n",
    "                                    reuse=True)\n",
    "    outputs = outputs.write(iteration, temp_preds)\n",
    "    seq_len = seq_len.write(iteration, temp_seq_len)\n",
    "    return iteration+1, outputs, temp_hid_state,seq_len,fake_decoder_input       ##,temp_embed_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally run the while loop. We dont need the latest hidden state and the iteration count, the only thing needed is the predictions of the decoder and the sequence length of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_while_teacher_f():\n",
    "    decoder_inputs_tr = tf.transpose(_target_notes)\n",
    "    decoder_input_arr = tf.TensorArray(dtype=tf.int32, size=max_notes_len+1,clear_after_read=False)\n",
    "    decoder_input_arr = decoder_input_arr.unstack(decoder_inputs_tr)\n",
    "    return tf.while_loop(decoder_condition, decoder_body_teacher_forcing, \n",
    "                    [init_i, init_outputs, init_hidden_state,init_seq_len,decoder_input_arr])\n",
    "def decoder_while_test():\n",
    "    return tf.while_loop(decoder_condition, decoder_body_test, \n",
    "                                [init_i, init_outputs, init_hidden_state,init_seq_len,1.0])\n",
    "_, dec_preds, _ ,seq_len,_= tf.cond(pred=_is_training, \n",
    "                                  true_fn=decoder_while_teacher_f,\n",
    "                                  false_fn=decoder_while_test\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, dec_preds contains the predictions for each word. So each item in the tensorarray contains (batch_size, vocab_size) shaped tensors. \n",
    "\n",
    "Now we are going to create a one tensor whose shape is (batch, max_notes_len,vocab_size). To achieve it, we are going to stack the dec_preds, which results in (max_notes_len, batch_size, vocab_size). Then transpose to get (batch_size,max_notes_len,vocab_size). And the same is done for the seq_len, except the we want seq_len as shape of (batch_size, max_notes_len). \n",
    "\n",
    "The all_seq_len variable will be used as a mask to the backpropagation. Because if it is 0, then the word is a < PAD >. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tf.transpose(dec_preds.stack(), [1,0,2])\n",
    "all_seq_len = tf.to_float(tf.transpose(seq_len.stack()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and the Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the mask for the backpropagation. If the target is < PAD > then don't backpropagate. To do it, we use all_seq_len variable, which already contains data for sequence length for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cost\"):\n",
    "    cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds, labels=_target_notes) * all_seq_len\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(_target_notes,\n",
    "                                  tf.argmax(preds, 2,output_type=tf.int32))\n",
    "    accuracy = tf.reduce_mean(tf.to_float(correct_prediction)) *100\n",
    "    acc_summary = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "### mean of the cross entropy is the loss of this batch\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(cross_ent)\n",
    "    loss_summary = tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_step = optimizer.minimize(cross_ent)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code Session\n",
    "Since we are using a Jupyter Notebook, we are going to use an InteractiveSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line if you did not initialize the sess before.\n",
    "#sess.close()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the data in the correct format. produce_batch returns the data along with its sequence length in the output. Note that, preoduce_batch returns data in time major order whose size (max_len,all_size).\n",
    "\n",
    "The sequence length of the the decoder targets is not needed, since it is calculated during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from preprocess import *\n",
    "encoder_inputs_, encoder_input_lengths_ = produce_batch(transformed_data)\n",
    "decoder_targets_, _ = produce_batch(transformed_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data and the sequence length of encoder input to train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_seqlens, test_seqlens, train_y, test_y = \\\n",
    "    train_test_split(encoder_inputs_.T, encoder_input_lengths_, decoder_targets_.T, \n",
    "                     test_size=test_train_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create randomized batches from all the data at hand. An important detail here is that, size of each batch should be batch_size, less than that is not allowed. Also, the batches are created directly as feed_dict for the session to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batches(x,sequence_len, y, batch_size):\n",
    "    assert len(x) == len(y)\n",
    "    shuffeled_pairs = list(zip(x,sequence_len,y))\n",
    "    np.random.shuffle(shuffeled_pairs)\n",
    "    x = np.array([i[0] for i in shuffeled_pairs])\n",
    "    sequence_len = np.array([i[1] for i in shuffeled_pairs])\n",
    "    ### In this line, we are removing the first words from the sentences which are all <go>\n",
    "    ### elements. Since we are providing that words by default, we dont need(want) them.\n",
    "    y = np.array([i[2] for i in shuffeled_pairs])[:,1:]\n",
    "    res = []\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        x_batch = x[i:min(i + batch_size, len(x))]\n",
    "        seq_len_batch = sequence_len[i:min(i + batch_size, len(sequence_len))]\n",
    "        y_batch = y[i:min(i + batch_size, len(y))]\n",
    "        \n",
    "        if len(x_batch) % batch_size == 0:\n",
    "            res.append({\n",
    "                _encoder_inputs: x_batch,\n",
    "                _encoder_seq_len: seq_len_batch,\n",
    "                _target_notes: y_batch,\n",
    "                _is_training:True\n",
    "            })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving and Checkpoint\n",
    "Here the summaries are merged, and the test and training log writes are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter(os.path.join(SAVE_PATH + \"/train\"),\n",
    "                                     graph=tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter(os.path.join(SAVE_PATH + \"/test\"),\n",
    "                                     graph=tf.get_default_graph())\n",
    "\n",
    "all_summary_train = tf.summary.merge([acc_summary, loss_summary])\n",
    "all_summary_test = tf.summary.merge([acc_summary, loss_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a pretrained model, import it for further use. If a model is found, one may actually skip the training part(which is the next code block after this import) to directly get some composition. If you would like to train a new model, and skip the existing model, you can run\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "and skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty logs/ directory when training on new data, or training a new model\n",
    "if glob.glob(SAVE_PATH + '*.meta'):\n",
    "    print(\" restoring an old model and training it further \")\n",
    "    imported_meta = tf.train.import_meta_graph(glob.glob(SAVE_PATH + '*.meta')[0])\n",
    "    imported_meta.restore(sess, tf.train.latest_checkpoint(SAVE_PATH))\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Building model from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "train_loss_track = []\n",
    "test_loss_track = []\n",
    "evaluate_every = 8\n",
    "for epoch in range(epochs):\n",
    "    batches = generate_batches(train_x, train_seqlens,train_y, batch_size)\n",
    "    if epoch == 0:\n",
    "        print(f\"Number of batches: {len(batches)}\")\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for e, batch in enumerate(batches) :\n",
    "        feed_dict = batch\n",
    "        _, train_loss,train_summary = sess.run([train_step, loss,all_summary_train], feed_dict)\n",
    "        train_writer.add_summary(train_summary)\n",
    "        train_loss_track.append(train_loss)\n",
    "        \n",
    "    saver.save(sess, os.path.join(SAVE_PATH, \"model\"))\n",
    "    test_batch = generate_batches(test_x, test_seqlens,test_y, batch_size)\n",
    "    test_loss,test_summary = sess.run([ loss,all_summary_test], test_batch[0])\n",
    "    test_loss_track.append(test_loss)\n",
    "    print(f\"Iterations trained:{len(batches)*epoch + e+1 }, Avg. Train Loss: {np.mean(train_loss_track[-batch_size:])}, Test Loss:{test_loss}\")\n",
    "    test_writer.add_summary(test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(train_loss_track)\n",
    "fig.suptitle('Train Loss', fontsize=20)\n",
    "plt.xlabel('Iteration', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(test_loss_track)\n",
    "fig.suptitle('Test Loss', fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compose function is to get a composition given a lyrics line. It would be better if the words in the input dictionary (data_dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates batches out of the test data and predicts the outputs\n",
    "def compose(lyrics):\n",
    "    lyrics_list, mask = prepare_prediction(lyrics, batch_size, data_dict)\n",
    "    pred_ = sess.run([preds],\n",
    "        feed_dict={\n",
    "            _encoder_inputs: lyrics_list,\n",
    "            _encoder_seq_len: [sum(i >0 for i in seq) for seq in lyrics_list],\n",
    "            _is_training:False,\n",
    "            _target_notes:np.ones((batch_size,target_dict.max_length+1))\n",
    "        })\n",
    "    \n",
    "    pred = np.argmax(pred_[0], axis=2)\n",
    "    print(np.array(pred).shape)\n",
    "    pred = [target_dict.reverse_transform(i) for i in pred]\n",
    "    return pred, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some lyrics to get composition :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input some test data\n",
    "lyrics = [\"my life is great because i love you\", \"you left me sad and i cry to bed every night\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the predicted music sequences could be listened. The time interval between notes/chords are currently not considered by the model. So we choose it as 0.5 seconds, which is a common choice. But one could also try randomizing the offset. We implemented this logic, where the offset is a random number between 0.4 and 0.6 seconds, which could be activated when calling notes_to_midi function by setting randomize_offset parameter to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the predicted notes\n",
    "pred, mask = compose(lyrics)\n",
    "for e, p in enumerate(pred):\n",
    "    if e < batch_size-mask:\n",
    "        if len(p) > 0:\n",
    "            print(f\"Prediction: {lyrics[e]}\")\n",
    "            print(p)\n",
    "            fname = f\"sample_{lyrics[e]}\"\n",
    "            notes_to_midi(p, fname)\n",
    "            play_midi(f\"output/{fname}.mid\")\n",
    "        else:\n",
    "            print(f\"Could not predict: {lyrics[e]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting graph could be seen by running the tensorboard on this project, with \n",
    "\n",
    "tensorboard --logdir=/path/to/this/project/logs/\n",
    "\n",
    "When we run it, we see the following graph generated:\n",
    "![title](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The trained model is able to generate music that forms to some extend a valid sequence. However the lyrics-notes matching challenge remains. From the loss graphs it is clear to us how the nature of the problem is really difficult. The loss decreased a lot at the beginning, whiich is a clear indicator that the model is clearly learning something, but towards the end, it keeps fluctuating, which indicates that the model cannot do better in learning the relation. This is is also intuitive because the use of the same word could be used in different connotations.\n",
    "\n",
    "\n",
    "### Future Work\n",
    "For future work we recommend the following:\n",
    "\n",
    "1) Try out a larger dataset, maybe manually annotate any form of music sequence to the lyrics. That way we are not restricted to the lyrics in the midi files.\n",
    "\n",
    "2) Include more information from the midi file instead of using the piano track solely. Maybe the offset sequence (the pauses between the notes) is an interesting factor in generating music. Also other instruments (drums e.g.) might be of much influence. To examine this further, we tried to add random pauses in the range of [0.4-0.6] in order to examine if it makes a difference and indeed it does. Here is an example, that shows how much if a difference the offset does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random output from a \n",
    "notes = ['4.6', 'C#4', 'F#4', 'B3', 'B3', '9', 'E4', 'F4', '6.9.10.11.0', '9.10.11.0.4', '6.9.10.11', '3.5', '1.6', '5.7.0', '11.1.3.6', '2.4.6.7.9.10.11', '7.11.2', '10.11.0.2.6', '2.6.7.9', '2.6.7.9', '8.9.11.1.4', 'F#3', '6.9.11', '8.9.11.1.4', 'A1', '3.6', '9.10.11', '6.9.11', '11.0.4', '6.10.11.1', '10.11.2.3.6', '10.11.0.2.6', '9', '4.6.9.10.11.0', '6.9.10.11.0', '0.5', '2.3.6.7.10.11', 'E4', 'G3', 'G#4', 'E4', 'E4', '6.7.10.11.2', '9.11', 'D5', '8.10', 'G#4']\n",
    "print(\"Without random offsets\")\n",
    "f = \"sample_without_offset\"\n",
    "notes_to_midi(notes, f)\n",
    "play_midi(f\"output/{f}.mid\")\n",
    "\n",
    "print(\"With random offsets\")\n",
    "f = \"sample_with_offset\"\n",
    "notes_to_midi(notes, \"sample_with_offset\", randomize_offset=True)\n",
    "play_midi(f\"output/{f}.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Try out a bigger model that might be able to exploit more structure in the input sequences, (could not try bigger models due to computational resources).\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
